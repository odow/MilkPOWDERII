{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting the Fonterra Farmgate Milk Price\n",
    "\n",
    "Oscar Dowson*, Department of Engineering Science, University of Auckland\n",
    "\n",
    "*odow003@aucklanduni.ac.nz\n",
    "\n",
    "## Introduction\n",
    "\n",
    "New Zealand is the world's largest exporter of dairy commodity products, representing approximately one third of international dairy trade. The contemporary New Zealand milk processing sector is dominated Fonterra, a large processing co-operative who collect 85% of the milk produced in New Zealand. Fonterra collect milk from their 10,500 farmer shareholders and process it into a range of commodity products (such as whole milk powder), as well as value-added products such as specialty cheeses, yoghurts and icecream.\n",
    "\n",
    "Suppliers on a basis of kilogram of milk solids (milkfat plus protein) supplied. However, due to the de-facto monopsony of Fonterra in New Zealand, and the small domestic market relative to production (95% is exported), there is no \"fair market price\" for milk produced in New Zealand. Therefore, the New Zealand Commerce Comission regulates the process by which Fonterra calculates the price it pays its farmer shareholders. This is broken into two components: the Farmgate Milk Price (FMP) and the dividend.\n",
    "\n",
    "The FMP corresponds to the international price that an efficient producer of dairy commodity products could achieve on the international market, while the dividend corresponds to the return on capital that the farmers have invested in Fonterra in order to produce added value products.\n",
    "\n",
    "The FMP and dividend are determined \\emph{ex-post} at the end of each season (which begins on June 1 each year). Therefore, during the season, farmers are uncertain about the price they will receive for milk they will produce during the season. Fonterra publish a forecast for the FMP at least every quarter, but historically these forecasts had large uncertainties of up to $\\pm50\\%$.\n",
    "\n",
    "The goal of this paper is to create a probabilistic model for the FMP. The FMP was chosen over the combined Farmgate Milk Price plus Dividend, because the FMP is the major component of the total price farmers receive (as a rule of thumb, the split between the FMP and dividend is on the order of of a 10 to 1). It is also the easier part to model since it is largely driven by (mostly) publically available information and a regulated process. In contrast, the dividend is derived from confidential internal sales figures and can be set by the board of Fonterra based on political and business decisions.\n",
    "\n",
    "**Global Dairy Trade**\n",
    "\n",
    "Since the very beginning, the international dairy commodity market has been dominated by regulation, protectionism and opaque price discovery. A key reason for this was the perishable nature of milk products, and strong domestic consumption in the milk producing regions of the world. In short, because most milk was drunk at home in the form of liquid milk or processed products like cheese and yoghurt, there was little incentive to develop well-functioning international commodity markets.\n",
    "\n",
    "However, as producers with liberal economic ideologies like New Zealand grew to dominate the international export market (representing a third of international exports despite producing only 3% of the total volume and exporting 95% of their domestic production), they began to want a new mechanism to discover the ''true'' international price of milk products.\n",
    "\n",
    "For these reasons, Fonterra established Global Dairy Trade (GDT) as an independent subsidiary in 2008. GDT is an international auction based marketplace for dairy commodity products. The platform positioned itself as an independent, trusted and transparent trading platform and has in many ways exceeded all expectations. It has become the de-facto reference price for dairy related commodities.\n",
    "\n",
    "The GDT auction is as an ascending price clock auction. For those readers who are not familiar with an ascending price clock auction, it begins with the sellers submitting an offer quantity (supply) and a price is set at which it is expected there will be more demand than the available the offer quantity. After being notified of the initial price, buyers submit demand quantities for each product. If this quantity exceeds the total available supply then, in a series of rounds, the price is slowly incremented, and buyers are given the opportunity to reduce the quantity they wish to purchase.  The auction ends when the total demand of the buyers is equal to the sellersâ€™ offer quantity (set at the open of the auction). All buyers pay the final price.\n",
    "\n",
    "Since milk is a perishable product, sellers are guaranteed to find a buyer for almost (depending on the size of the price increment) all the milk they wish to sell. Furthermore, since buyers can only decrease their purchase quantity, they must enter the auction demanding close to what they want to purchase.\n",
    "\n",
    "There are 24 auctions each year. These are held on the first and third Tuesdays of each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cells like this contain code that is used to analyse the data.\n",
    "# You need to install Julia v0.5 and the IJulia.jl package if you want to execute them yourself\n",
    "# (You'll also need the data...)\n",
    "\n",
    "# Lets load some helpful packages\n",
    "#  For dealing with tabular data\n",
    "using DataFrames, DataFramesMeta\n",
    "#  Interacting with the web\n",
    "using JSON, Requests\n",
    "#  Plotting\n",
    "using Plots, StatPlots, PlotRecipes, RecipesBase\n",
    "#  and doing some proper statistics\n",
    "using Distributions, GLM, StatsBase\n",
    "\n",
    "# We're also going to create some helper functions to simplify the path structure\n",
    "function getdata(s, datekey=nothing, datestr=\"yyyy-mm-dd\")\n",
    "    df = readtable(joinpath(dirname(pwd()), \"data\", s))\n",
    "    if !(datekey == nothing)\n",
    "        df[datekey] = convert(DataArray{Date, 1}, map(d->Dates.Date(d, datestr), df[datekey]))\n",
    "    end\n",
    "    df\n",
    "end\n",
    "modelpath(s) = joinpath(dirname(pwd()), \"data/model_parameters\", s)\n",
    "datapath(s) = joinpath(dirname(pwd()), \"data\", s)\n",
    "docpath(s) = joinpath(dirname(pwd()), \"docs\", s)\n",
    "\n",
    "# And we're going to use this function to de-serialise a DataFrame from JSON.\n",
    "function load_df_from_json(d::Dict)\n",
    "    df = DataFrame(hcat(d[\"columns\"]...))\n",
    "    names!(df, map(Symbol, d[\"colindex\"][\"names\"]))\n",
    "    df\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function stripsuffix(s)\n",
    "    for suf in [\"st\", \"nd\", \"rd\", \"th\"]\n",
    "        s = replace(s, suf, \"\")\n",
    "    end\n",
    "    s\n",
    "end\n",
    "function grablatestagrihq()\n",
    "    SEASON = 2017\n",
    "    nzxdata = JSON.parse(String(get(\"https://dairy-tools.nzx.com/fgmp/calculator_widget.json?client_key=15491abd-1307-4caa-abaa-843bc827e8eb\").data))\n",
    "    fgmp = 0.0\n",
    "    effective_date = Dates.today()\n",
    "    for calc in nzxdata[\"calculations\"]\n",
    "        if calc[\"calculation_type\"] == \"forecast\" || calc[\"calculation_type\"] == \"estimate\"\n",
    "            fgmp += calc[\"fgmpe\"] * calc[\"rwmp\"]\n",
    "            effective_date = Dates.Date(stripsuffix(calc[\"effective_at\"]), \"U d, y\")\n",
    "        end\n",
    "    end\n",
    "    aghq = getdata(\"forecasts/agrihq.csv\", :Date)\n",
    "    append!(aghq, DataFrame(Date = effective_date, Season=SEASON, Forecast= round(fgmp, 2)))\n",
    "    unique!(aghq)\n",
    "    writetable(datapath(\"forecasts/agrihq.csv\"), aghq)\n",
    "end\n",
    "try\n",
    "    grablatestagrihq()\n",
    "catch\n",
    "    warn(\"Unable to get latest agrihq data\")\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2string(url) = String(get(url).data)\n",
    "gettables(page) = matchall(r\"<table.*?>(.+?)</table>\"s, page)\n",
    "getrows(table) = matchall(r\"<tr.*?>(.+?)</tr>\"s, table)\n",
    "getcells(row) = matchall(r\"<td.*?>(.+?)</td>\"s, row)\n",
    "getcellcontent(cell) = strip(match(r\"<td.*?>(.+?)</td>\"s, cell)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function extractforecasts(table, product)\n",
    "    rows = getrows(table)\n",
    "    trading_periods = [getcellcontent(cell) for cell in getcells(rows[2])]\n",
    "    trading_periods[5] = \"12 Month\"\n",
    "    Event   = String[]\n",
    "    CP      = Int[]\n",
    "    Quantity = String[]\n",
    "    for (cp, row) in enumerate(rows[5:10])\n",
    "        for (i, cell) in enumerate(getcells(row)[1:5])\n",
    "            push!(Event, trading_periods[i])\n",
    "            push!(CP, cp)\n",
    "            push!(Quantity, getcellcontent(cell))\n",
    "        end\n",
    "    end\n",
    "    return DataFrame(\n",
    "        Date=fill(Dates.today(), length(CP)),\n",
    "        Event = Event,\n",
    "        CP = CP,\n",
    "        Product = fill(product, length(CP)),\n",
    "        Quantity= Quantity,\n",
    "        Type = fill(\"Forecast\", length(CP))\n",
    "    ), trading_periods\n",
    "end\n",
    "\n",
    "function extractactuals(table, product, event)\n",
    "    rows = getrows(table)\n",
    "    DataFrame(\n",
    "        Date=fill(Dates.today(), 6),\n",
    "        Event = fill(event, 6),\n",
    "        CP = 1:6,\n",
    "        Product = fill(product, 6),\n",
    "        Quantity = [String(getcellcontent(cell)) for cell in getcells(rows[4])],\n",
    "        Type = fill(\"Actual\", 6)\n",
    "    )\n",
    "end\n",
    "function getactualofferquantity(product)\n",
    "    page = url2string(\"http://www.nzxfutures.com/dairy/events/GDT$(product)/details\")\n",
    "    tables = gettables(page)\n",
    "    tableindex = (length(tables) == (product==\"SMP\"?4:3))?2:1\n",
    "    (forecasts, trade_events) = extractforecasts(tables[tableindex], product)\n",
    "    if tableindex == 2        \n",
    "        event = \"TE $(parse(Int, trade_events[1][(end-2):end]) - 1)\"\n",
    "        append!(forecasts, extractactuals(tables[1], product, event))\n",
    "    end\n",
    "    forecasts\n",
    "end\n",
    "function updatequantityforecats()\n",
    "    df = getactualofferquantity(\"WMP\")\n",
    "    for product in [\"BMP\", \"BUT\", \"SMP\", \"WMP\"]\n",
    "        append!(df, getactualofferquantity(product))\n",
    "    end\n",
    "    df[:Quantity] = convert(DataArray{Int}, map(d->parse(Int, replace(d, \",\", \"\")), df[:Quantity]))\n",
    "    df[:CP] = convert(DataArray{String}, map(s->\"$s\", df[:CP]))\n",
    "    stored_df = getdata(\"fonterra/forecasts.csv\", :Date)\n",
    "\n",
    "    for row in 1:size(df, 1)\n",
    "        tmp_row = @where(stored_df, :Event .== df[row, 2], :CP .== df[row, 3], :Product .== df[row, 4], :Type .== df[row, 6])\n",
    "#         if size(tmp_row, 1) == 0\n",
    "            # new forecast\n",
    "            append!(stored_df, df[row, :])\n",
    "#         elseif tmp_row[end, :Quantity] != df[row, :Quantity]\n",
    "            # updated forecast\n",
    "            append!(stored_df, df[row, :])\n",
    "#         end\n",
    "    end\n",
    "    unique!(stored_df)\n",
    "    sort!(stored_df, cols=[:Date, :Product, :Event, :CP])\n",
    "    writetable(datapath(\"fonterra/forecasts.csv\"), stored_df)\n",
    "    writetable(docpath(\"data/fonterra_quantity_forecasts.csv\"), stored_df)\n",
    "end\n",
    "try\n",
    "    updatequantityforecats()\n",
    "catch\n",
    "    warn(\"Failed to update quantity forecasts\") \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function getnthtuesday(year, month, n)\n",
    "    for day in 1:7\n",
    "        d = Dates.Date(year, month, day + (n-1)*7) \n",
    "        if Dates.dayofweek(d) == 2\n",
    "            return d\n",
    "        end\n",
    "    end\n",
    "    return Dates.Date(year, month, 1)\n",
    "end\n",
    "function getdateofauction(n)\n",
    "    te183 = Dates.Date(2017,3,1)\n",
    "    nth = isodd(n)?1:3\n",
    "    months = floor(Int, (n - 183)/2)\n",
    "    te_nth = te183 + Dates.Month(months)\n",
    "    getnthtuesday(Dates.year(te_nth), Dates.month(te_nth), nth)\n",
    "end\n",
    "\n",
    "function gettotalforecasts()\n",
    "    forecasts = getdata(\"fonterra/forecasts.csv\", :Date)\n",
    "    forecasts[:Event] = map(x->begin\n",
    "        if x[1:2] != \"TE\" && x != \"12 Month\"\n",
    "            return \"TE $x\"\n",
    "        else\n",
    "            return x\n",
    "        end \n",
    "    end, forecasts[:Event])\n",
    "\n",
    "    sort!(forecasts, cols=[:Product, :Event, :Date])\n",
    "    total_forecasts = @select(by(@where(forecasts, :CP .!= \"Total\"), [:Product, :Event, :Type]) do df\n",
    "        _df_ = by(df, :CP) do _\n",
    "            DataFrame(\n",
    "            Quantity=_[:Quantity][end],\n",
    "            Date=_[:Date][end]\n",
    "            )\n",
    "        end\n",
    "        DataFrame(\n",
    "            Quantity=sum(_df_[:Quantity]),\n",
    "            Date=maximum(_df_[:Date]),\n",
    "            CP = \"Total\"\n",
    "        )\n",
    "        end, :Date, :Event, :CP, :Product, :Quantity, :Type)\n",
    "    append!(total_forecasts, forecasts)\n",
    "    unique!(total_forecasts)\n",
    "    sort!(total_forecasts, cols=[:Date, :Product, :Event])\n",
    "    total_forecasts = @where(total_forecasts, :Event .!= \"12 Month\", :CP .== \"Total\")\n",
    "    total_forecasts[:TE] = convert(DataArray{Int}, map(x->parse(Int, x[4:end]), total_forecasts[:Event]))\n",
    "    total_forecasts[:AuctionDate] = map(getdateofauction, total_forecasts[:TE])\n",
    "    total_forecasts\n",
    "end\n",
    "function getdfforecast(total_forecasts)\n",
    "    dfforecast = join(@where(total_forecasts, :Type .== \"Actual\"), @where(total_forecasts, :Type .==\"Forecast\"), on=[:Product, :CP, :Event, :TE, :AuctionDate])\n",
    "    rename!(dfforecast, :Date, :Actual_Date)\n",
    "    rename!(dfforecast, :Date_1, :Forecast_Date)\n",
    "    rename!(dfforecast, :Quantity, :Actual_Quantity)\n",
    "    rename!(dfforecast, :Quantity_1, :Forecast_Quantity)\n",
    "    delete!(dfforecast, :Type_1)\n",
    "    dfforecast[:TEPrior] = map((a,b)->floor(Int, round(Int, (a-b).value / 7)/ 2+0.1), dfforecast[:AuctionDate], dfforecast[:Forecast_Date])\n",
    "    # dfforecast[:RelativeError] = (dfforecast[:Actual_Quantity] ./ dfforecast[:Forecast_Quantity] - 1)*100\n",
    "    dfforecast[:ActualError] = dfforecast[:Actual_Quantity] - dfforecast[:Forecast_Quantity]\n",
    "    sort!(dfforecast, cols=[:Product, :Forecast_Date, :Event, :CP])\n",
    "    dfforecast\n",
    "end\n",
    "function saveforecastquantities()\n",
    "    total_forecasts = gettotalforecasts()\n",
    "    dfforecast = getdfforecast(total_forecasts)\n",
    "    data = Dict()\n",
    "    for prod in [\"WMP\", \"SMP\"]\n",
    "        prod_df = @where(dfforecast, :Product .== prod)\n",
    "        data[prod] = Dict()\n",
    "        tmp_df = @select(@where(prod_df, :Type .== \"Actual\"), :TE, :Actual_Date, :Actual_Quantity)\n",
    "        unique!(tmp_df)\n",
    "        data[prod][\"actual\"] = Dict(\"date\" => tmp_df[:Actual_Date], \"quantity\" => tmp_df[:Actual_Quantity])\n",
    "\n",
    "        for te in 1:4\n",
    "            tmp_df = @where(prod_df, :TEPrior.==te)\n",
    "            data[prod][\"te$(te)\"] = Dict(\n",
    "                \"date\" => tmp_df[:Actual_Date],\n",
    "                \"error\" => map(x->round(x, 2), tmp_df[:ActualError]),\n",
    "                \"quantity\" => tmp_df[:Forecast_Quantity]\n",
    "            )\n",
    "        end\n",
    "    end\n",
    "    open(docpath(\"json/forecast_quantity.json\"), \"w\") do file\n",
    "        print(file, \"forecast_quantity = \")\n",
    "        JSON.print(file, data)\n",
    "        println(file, \";\")\n",
    "    end\n",
    "end\n",
    "try\n",
    "    saveforecastquantities()\n",
    "catch\n",
    "    warn(\"Error saving json/forecast_quantity.json\")\n",
    "end\n",
    "\n",
    "function getcombineddf()\n",
    "    gdt = getdata(\"gdt/events.csv\", :Date)\n",
    "    gdtprice = melt(gdt, [:Date, :Season, :TE])\n",
    "    rename!(gdtprice, Dict(:Date=>:AuctionDate, :variable=>:Product, :value=>:Price))\n",
    "    gdtprice[:Product] = map(s->\"$s\", gdtprice[:Product])\n",
    "\n",
    "    total_forecasts = gettotalforecasts()\n",
    "    dfforecast = getdfforecast(total_forecasts)\n",
    "\n",
    "    combined_df = join(gdtprice, @where(dfforecast, :CP.==\"Total\"), on=[:AuctionDate, :Product, :TE])\n",
    "    combined_df\n",
    "end\n",
    "function rungdtactualprices()\n",
    "    combined_df = getcombineddf()\n",
    "#     maxdate = maximum(@where(combined_df, :Type .== \"Actual\")[:AuctionDate])\n",
    "    data = Dict()\n",
    "    for prod in [\"WMP\", \"SMP\"]\n",
    "        tmp_df = @select(@where(combined_df, :Product .== prod, :Type .== \"Actual\"), :AuctionDate, :Actual_Quantity, :Price)\n",
    "        unique!(tmp_df)\n",
    "        data[prod] = Dict(\"date\" => tmp_df[:AuctionDate], \"price\" => tmp_df[:Price])\n",
    "    end\n",
    "    open(docpath(\"json/actual_gdt_prices.json\"), \"w\") do file\n",
    "        print(file, \"actual_gdt_prices = \")\n",
    "        JSON.print(file, data)\n",
    "        println(file, \";\")\n",
    "    end\n",
    "end\n",
    "\n",
    "try\n",
    "    rungdtactualprices()\n",
    "catch\n",
    "    warn(\"Error saving json/actual_gdt_prices.json\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_forecasts = gettotalforecasts()\n",
    "dfforecast = getdfforecast(total_forecasts)\n",
    "@where(dfforecast, :Event .== \"TE 185\", :Product .== \"SMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function savefonterraforecastquantities()\n",
    "    combined_df = getcombineddf()\n",
    "    total_forecasts = gettotalforecasts()\n",
    "    historic_accuracies = by(@where(combined_df, :Actual_Date .>=Dates.Date(2013,1,1)), [:Product, :TEPrior]) do df\n",
    "        DataFrame(\n",
    "        lower = quantile(df[:ActualError], 0.1),\n",
    "        middle = quantile(df[:ActualError], 0.5),\n",
    "        upper = quantile(df[:ActualError], 0.9)\n",
    "        )\n",
    "    end\n",
    "    data = Dict()\n",
    "    for prod in [\"WMP\", \"SMP\"]\n",
    "        latestdate = maximum(total_forecasts[:Date])\n",
    "        tmp_df = @where(total_forecasts, :AuctionDate .>= Dates.today(), :Type .== \"Forecast\", :Product .== prod)\n",
    "        tmp_df = by(tmp_df, :Event) do _\n",
    "            _[end, :]\n",
    "        end\n",
    "        @show tmp_df\n",
    "        tmp_df = tmp_df[(end-3):end, :]\n",
    "\n",
    "        tmp_df[:TEPrior] = 1:4\n",
    "        tmp_df = join(tmp_df, historic_accuracies, on=[:Product, :TEPrior])\n",
    "        data[prod] = Dict(\n",
    "        \"date\"   => tmp_df[:AuctionDate],\n",
    "        \"lower\"  => map(x->round(Int, x), tmp_df[:Quantity] + tmp_df[:lower]),\n",
    "        \"middle\" => map(x->round(Int, x), tmp_df[:Quantity] + tmp_df[:middle]),\n",
    "        \"upper\"  => map(x->round(Int, x), tmp_df[:Quantity] + tmp_df[:upper])\n",
    "        )\n",
    "    end\n",
    "    open(docpath(\"json/fonterra_forecast_quantity.json\"), \"w\") do file\n",
    "        print(file, \"fonterra_forecast_quantity = \")\n",
    "        JSON.print(file, data)\n",
    "        println(file, \";\")\n",
    "    end\n",
    "end\n",
    "try\n",
    "    savefonterraforecastquantities()\n",
    "catch\n",
    "    warn(\"Error saving json/fonterra_forecast_quantity.json\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function saveseasonalisedquantitypredictions()\n",
    "    combined_df = getcombineddf()\n",
    "    combined_df[:Month] = map(d->Dates.Month(d).value, combined_df[:Actual_Date])\n",
    "    tmp_df = @select(combined_df, :Product, :Month, :Actual_Quantity)\n",
    "    unique!(tmp_df)\n",
    "    sort!(tmp_df, cols=[:Product, :Month])\n",
    "    average_quantity = by(tmp_df, [:Product, :Month]) do df\n",
    "        DataFrame(\n",
    "            lower = quantile(df[:Actual_Quantity], 0.1),\n",
    "            middle = quantile(df[:Actual_Quantity], 0.5),\n",
    "            upper = quantile(df[:Actual_Quantity], 0.9)\n",
    "        )\n",
    "    end\n",
    "    data = Dict()\n",
    "    for prod in [\"WMP\", \"SMP\"] \n",
    "        data[prod] = Dict()\n",
    "\n",
    "        dates  = []\n",
    "        lower  = []\n",
    "        middle = []\n",
    "        upper  = []\n",
    "        for futurete in maximum(@where(combined_df, :Type .== \"Actual\")[:TE]) + (5:24)\n",
    "            dte = getdateofauction(futurete) \n",
    "            push!(dates, dte)\n",
    "            tb = @where(average_quantity, :Product .== prod, :Month .== Dates.month(dte))\n",
    "            push!(lower, round(Int, tb[:lower][1]))\n",
    "            push!(middle, round(Int, tb[:middle][1]))\n",
    "            push!(upper, round(Int, tb[:upper][1]))\n",
    "        end\n",
    "        data[prod][\"date\"] = dates\n",
    "        data[prod][\"lower\"] = lower\n",
    "        data[prod][\"middle\"] = middle\n",
    "        data[prod][\"upper\"] = upper\n",
    "    end\n",
    "    open(docpath(\"json/seasonalised_quantity.json\"), \"w\") do file\n",
    "        print(file, \"seasonalised_quantity = \")\n",
    "        JSON.print(file, data)\n",
    "        println(file, \";\")\n",
    "    end\n",
    "end\n",
    "try\n",
    "    saveseasonalisedquantitypredictions()\n",
    "catch\n",
    "    warn(\"Error saving json/seasonalised_quantity.json\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since not all plotting backends support ysticks, we're going to create our own using a recipe!\n",
    "@recipe function f(::Type{Val{:ysticks}}, x, y, z)\n",
    "    xnew, ynew = zeros(3 * length(x)), zeros(3 * length(x))\n",
    "    xnew[1:3:end] = x\n",
    "    xnew[2:3:end] = x\n",
    "    xnew[3:3:end] = NaN\n",
    "    \n",
    "    ynew[2:3:end] = y\n",
    "    ynew[3:3:end] = NaN\n",
    "    \n",
    "    seriestype  := :path\n",
    "    x           := xnew\n",
    "    y           := ynew\n",
    "    \n",
    "    @series begin\n",
    "        seriestype := :scatter\n",
    "        x := x\n",
    "        y := y\n",
    "        label := \"\"\n",
    "        primary := false\n",
    "        markershape := d[:markershape]\n",
    "        markerstrokewidth := d[:markerstrokewidth]\n",
    "        ()\n",
    "    end\n",
    "    markershape := :none\n",
    "    ()\n",
    "end\n",
    "Plots.@deps ysticks path scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same thing goes for a correlogram!\n",
    "@userplot correlogram\n",
    "@recipe function f(cor::correlogram; partial=true, zvalue=1.96)\n",
    "    x, lag = cor.args\n",
    "    x = reshape(x, (length(x), 1))\n",
    "    y = zeros(lag, 1)\n",
    "    conf_int = zeros(lag)\n",
    "\n",
    "    if partial\n",
    "        StatsBase.pacf!(y, x, collect(1:lag))\n",
    "        conf_int += zvalue / sqrt(length(x))\n",
    "    else\n",
    "        StatsBase.autocor!(y, x, collect(1:lag))\n",
    "        for i=1:lag\n",
    "            r = 0.\n",
    "            for j=1:i\n",
    "                r += y[j]^2\n",
    "            end\n",
    "            conf_int[i] = zvalue * sqrt(1 / length(x) * (1 + 2r))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    xguide --> \"Lag\"\n",
    "    yguide --> (partial?\"Partial Autocorrelation Coefficient\":\"Autocorrelation Coefficient\")\n",
    "    ylims  --> (-1, 1)\n",
    "    @series begin\n",
    "        seriestype  := :ysticks\n",
    "        markershape := :circle\n",
    "        markerstrokewidth := 0\n",
    "        x           := 1:lag\n",
    "        y           := y\n",
    "        label := \"\"\n",
    "        ()\n",
    "    end\n",
    "    @series begin\n",
    "        seriestype  := :line\n",
    "        x           := 1:lag\n",
    "        y           := conf_int\n",
    "        linestyle   := :dot\n",
    "        linecolor       := colorant\"darkgrey\"\n",
    "        label := \"\"\n",
    "        ()\n",
    "    end\n",
    "    @series begin\n",
    "        seriestype  := :line\n",
    "        x           := 1:lag\n",
    "        y           := -conf_int\n",
    "        linestyle   := :dot\n",
    "        label := \"\"\n",
    "        linecolor       := colorant\"darkgrey\"\n",
    "        ()\n",
    "    end\n",
    "end\n",
    "Plots.@deps correlogram ysticks line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a Q-Q plot!\n",
    "@userplot qqplot\n",
    "@recipe function f(qq::qqplot)\n",
    "    dist, xx = qq.args\n",
    "    Q = qqbuild(dist, xx)\n",
    "    xguide --> \"Theoretical Quantile\"\n",
    "    yguide --> \"Sample Quantile\"\n",
    "    @series begin\n",
    "        seriestype := :scatter\n",
    "        x := Q.qx\n",
    "        y := Q.qy\n",
    "        label := \"\"\n",
    "        markersize --> 2\n",
    "        markerstrokewidth --> 0\n",
    "        ()\n",
    "    end\n",
    "    \n",
    "    Z = [min(minimum(Q.qx), minimum(Q.qy)), max(maximum(Q.qx), maximum(Q.qy))]\n",
    "    @series begin\n",
    "        seriestype := :line\n",
    "        x := Z\n",
    "        y := Z\n",
    "        label := \"\"\n",
    "        ()\n",
    "    end\n",
    "end\n",
    "Plots.@deps qqplot scatter line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets load our plotting backend and set a nice theme to use\n",
    "pyplot()\n",
    "theme(:sand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonterra Forecasts\n",
    "\n",
    "Fonterra are in the best position to forecast the Farmgate Milk Price. They are privy to commercially sensitive data such as the exchange rate, product mix and sales curve that we have had to approximate. They do release a forecast at least every quarter (as mandated by DIRA 2001). In the graph below, we plot every forecast Fonterra has made over time (the x-axis). Each coloured line is a different season. The rightmost point on each line is the date and price at which the the Farmgate Milk Price was finalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First up, lets extract all the historic Fonterra forecasts\n",
    "# We're going to encapsulate all the analysis in functions to minimise the quantity of objects in the global scope.\n",
    "function get_fonterra_forecast()\n",
    "    # load the dataframe\n",
    "    fonterra_forecast = getdata(\"forecasts/fonterra.csv\", :Date, \"yyyy-m-d\")\n",
    "    # convert to Julia Dates\n",
    "#     fonterra_forecast[:Date] = convert(DataArray{Date, 1}, map(d->Dates.Date(d, \"yyyy-m-d\"), fonterra_forecast[:Date]))\n",
    "    # prettify the season string\n",
    "    fonterra_forecast[:sSeason] = map(s->\"$(s)/$(s - 2000 + 1)\", fonterra_forecast[:Season])\n",
    "    # calculate how many days since the start of the season (June 1st)\n",
    "    fonterra_forecast[:Day] = map((d, s) -> (d - Dates.Date(s, 6, 1)).value, fonterra_forecast[:Date], fonterra_forecast[:Season])\n",
    "    # sort based on date of the forecast\n",
    "    sort!(fonterra_forecast, cols=[:Date])\n",
    "    \n",
    "    return fonterra_forecast\n",
    "end\n",
    "\n",
    "# grab the historic data\n",
    "fonterra_forecast = get_fonterra_forecast()\n",
    "\n",
    "# Plot it!\n",
    "plot(fonterra_forecast,\n",
    "    :Date,\n",
    "    :Forecast,\n",
    "    group=:sSeason,\n",
    "    linetype=:step,\n",
    "    xlabel=\"Date of Forecast\",\n",
    "    ylabel=\"Forecast FMP (\\$/kgMS)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some seasons, such as the 2012/13 season, Fonterra's forecasts were close to the the final Farmgate Milk Price (i.e. within \\$0.50). However, in other seasons, such as the 2009/10 or 2014/15 season, Fonterra has be out by more than \\$2/kgMS in its initial forecast. In the graph below, we plot the error in Fonterra's forecasts against the number of days since the season began. We bound these errors by the maximum absolute error observed in the historical forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the irregularly spaced historic forecasts, we're going to interpolate them using the most recent value\n",
    "function get_daily_forecast(fonterra_forecast)\n",
    "    # Calculate the final payout (last forecast recorded)\n",
    "    final_payout = by(fonterra_forecast, :Season) do df\n",
    "        DataFrame(FinalPayout = df[:Forecast][end])\n",
    "    end\n",
    "    # Drop this current season\n",
    "    fonterra_forecast2 = join(@where(fonterra_forecast, :Season .< 2016), final_payout, on=[:Season])\n",
    "    # Calculate the additive error\n",
    "    fonterra_forecast2[:Error] = fonterra_forecast2[:Forecast] - fonterra_forecast2[:FinalPayout]\n",
    "    \n",
    "    # Calculate error for each season up to 500 days from 1 June\n",
    "    by(fonterra_forecast2, :Season) do df\n",
    "        # Initialise storage\n",
    "        y = zeros(500)    \n",
    "        # for each forecast\n",
    "        for j=1:(size(df, 1)-1)\n",
    "            day_start = max(1, df[:Day][j])\n",
    "            day_stop  = max(1, df[:Day][j+1]-1)\n",
    "            # interpolate\n",
    "            y[day_start:day_stop] = df[:Error][j]    \n",
    "        end\n",
    "        # store the last\n",
    "        y[df[:Day][end]:end] = df[:Error][end]\n",
    "        # return a DataFrame\n",
    "        DataFrame(day=1:500, Error=y)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Get the daily forecast\n",
    "daily_forecast = get_daily_forecast(fonterra_forecast)\n",
    "\n",
    "# Calculate the worst historic forecast at any given day\n",
    "function get_error_bars(daily_forecast)\n",
    "    by(daily_forecast, :day) do df\n",
    "        DataFrame(\n",
    "            PlusError=maximum(abs(df[:Error])),\n",
    "            NegError=-maximum(abs(df[:Error]))\n",
    "        )\n",
    "    end\n",
    "end\n",
    "\n",
    "# Get the worst forecast\n",
    "error_bars = get_error_bars(daily_forecast)\n",
    "\n",
    "# Plot it!\n",
    "#  the historic forecasts...\n",
    "plot(daily_forecast, :day, :Error, group=:Season)\n",
    "#  the positive error\n",
    "plot!(error_bars, :day, :PlusError, label=\"Maximum Abs. Error\", linewidth=2, c=colorant\"darkblue\")\n",
    "#  the negative error\n",
    "plot!(error_bars, :day, :NegError, label=\"\", linewidth=2, c=colorant\"darkblue\")\n",
    "#  fill in the plot labels\n",
    "plot!(xlabel=\"Days since June 1st\", ylabel=\"Error in Forecast (\\$/kgMS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start of the season, Fonterra can be out by as much as \\$2.50/kgMS. However, by the end of the season (i.e. in the following June), this error has narrowed to $\\pm$\\$0.2/kgMS. The reason for the residual error, even after the season has finished is the lag between production and sales, as well as a delay in finalising the average exchange rate achieved, and total costs. One may also conjecture that there is a political decision to delay the announcement of the final price until the annual results are announced in September, despite Fonterra internally being aware of a more accurate figure sometime between June and September.\n",
    "\n",
    "We therefore have a model for the FMP by assuming the current Fonterra forecast, plus or minus the calculated maximum absolute error. We assume that this error is uniformly distributed. One reason that a uniform distribution was chosen was due to the lack of available data (we only have seven seasons since Fonterra changed the mechanism by which they set the FMP). There is no evidence that more complicated models of the uncertainty (such as the errors being normally, or triangularly distributed) provide more predictive power. For a similar reason, we chose to use the empirical error function, rather than fitting a smooth error function to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the daily error\n",
    "const fonterra_uniform_error = collect(error_bars[:PlusError])\n",
    "\n",
    "# save it to JSON so we can use later\n",
    "open(modelpath(\"fonterra_error.json\"), \"w\") do file\n",
    "    JSON.print(file, map(r->round(r, 3), fonterra_uniform_error))\n",
    "end\n",
    "\n",
    "# This function creates a Uniform distribution for possible prices based on Fonterra's current forecast\n",
    "#    and the days_since_june_first\n",
    "function create_fonterra_distribution(forecast, days_since_june_first)\n",
    "    fonterra_uniform_error = JSON.parsefile(modelpath(\"fonterra_error.json\"))\n",
    "    err = fonterra_uniform_error[days_since_june_first]\n",
    "    Uniform(forecast-err, forecast+err)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Forecasters\n",
    "\n",
    "In addition to Fonterra, there are other analysts in the market who forecast the Farmgate Milk Price. Chiefly, these are the major banks since their lending criteria depends a lot on forecasting the return a given farmer will make. We currently record the forecasts of five major analysts who produce public forecasts on a semi-regular basis:\n",
    "\n",
    "1. [AgriHQ](https://agrihq.co.nz/toolbox/farmgate-milk-price-calculator/)\n",
    "2. [ASB](http://reports.asb.co.nz/report-list/channel/4035/0/0/rural-economic-note.html)\n",
    "3. [BNZ](https://www.bnz.co.nz/institutional-banking/research/publications#publication-Rural_Wrap)\n",
    "4. [NZX](http://www.nzxfutures.com/dairy/quotes/mkp)\n",
    "5. [Westpac](https://www.westpac.co.nz/agribusiness/agri-information/fortnightly-agri-updates/).\n",
    "\n",
    "We assume that each of these forecasts have the same historical uncertainty as the Fonterra forecast. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function getanalystforecast(analyst, season, date)\n",
    "    # look up the empirical error function\n",
    "    fonterra_uniform_error = JSON.parsefile(modelpath(\"fonterra_error.json\"))\n",
    "    # load the analysts forecasts\n",
    "    df = getdata(\"forecasts/$(analyst).csv\", :Date)\n",
    "    # convert the Date to Julian format\n",
    "#     df[:Date] = convert(DataArray{Date}, map(i->Date(i, \"yyyy-mm-dd\"), df[:Date]))\n",
    "    # get the most recent forecast\n",
    "    forecast = @where(df, :Season.==season, :Date .<= date)[:Forecast][end]\n",
    "    # calculate the additive error\n",
    "    err = fonterra_uniform_error[(date - Dates.Date(season, 6,1)).value]\n",
    "    # return probability distribution\n",
    "    return Uniform(forecast-err, forecast+err)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Farmgate Milk Price as a weighted sum of GDT auctions\n",
    "\n",
    "In actuality, Fonterra calculate the Farmgate Milk Price as the weighted sum of the GDT auctions, converted into New Zealand dollars at an average exchange rate ($r$), less the costs incurred in production ($c$). \n",
    "\n",
    "$$FMP = \\frac{\\sum\\limits_{t=1}^{T}\\sum\\limits_{i\\in I} w_t \\lambda_{t,i} p_{t,i}}{r} - c,$$\n",
    "\n",
    "where $I = \\{AMF, BUT, BMP, SMP, WMP\\}$, $w_t$ is the proportion of production sold in GDT auction $t$, $\\lambda_{t,i}$ is the quantity of RCP $i$ produced in GDT auction period $t$ from 1kgMS, and $p_{t,i}$ is the average winning price for RCP $i$ in GDT auction $t$.\n",
    "\n",
    "We then face five challenges\n",
    "\n",
    "1. Estimating the sales curve $w_t$\n",
    "2. Estimating the product mix $\\lambda_{t,i}$\n",
    "3. Forecasting the average exchange rate $r$\n",
    "4. Forecasting the costs incurred by Fonterra $c$\n",
    "5. Forecasting GDT Prices $p_{t,i}$\n",
    "\n",
    "In the following sections we investigate each of these in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Estimating the Sales Curve\n",
    "\n",
    "The second important factor to consider is the timing of Fonterra's sales. In the Farmgate Milk Price Statement, Fonterra only provides sales data broken down by quarter. This proved to be too coarse for our analysis. However, Fonterra provide the cumulated contracted sales volume on a monthly basis in a graph format. This data was estimated from the graph and recorded as the approximate cumulative monthly contracts sales volume (in thousand metric tonnes) for the seasons 2011/12 through 2015/16. There is a small lag between supply, production and sales so that it is not until July of the following season (i.e. two months into the next season) that the final sales of the previous seasons production have been completed.\n",
    "\n",
    "We can calculate the average sales curve my taking the mean of the cumulative proportion of total sales achieved in each month over the last five seasons and then calculating the difference between months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the monthly sales data\n",
    "theme(:none)\n",
    "function get_sales_data()\n",
    "    df = getdata(\"fonterra/monthly_contracts.csv\")\n",
    "    # rename some columns\n",
    "    names!(df, [:Month, Symbol(\"2011/12\"), Symbol(\"2012/13\"), Symbol(\"2013/14\"), Symbol(\"2014/15\"), Symbol(\"2015/16\")])\n",
    "\n",
    "    # go from cumulative to proportional\n",
    "    for i=reverse(2:size(df, 1))\n",
    "        for j in 2:size(df, 2)\n",
    "            df[i, j] = df[i, j] - df[i-1, j]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    df = melt(df, :Month)\n",
    "    df = join(df, by(df, :variable) do _df\n",
    "        sum(_df[:value])\n",
    "        end, on=[:variable])\n",
    "    df[:Proportion] = df[:value] ./ df[:x1]\n",
    "    df = @select(df, :Month, :Proportion, :variable)\n",
    "    rename!(df, :variable, :Season)\n",
    "    df\n",
    "end\n",
    "sales_data = get_sales_data()\n",
    "\n",
    "unique_months = unique(sales_data[:Month])\n",
    "\n",
    "function get_sales_curve(sales_data, unique_months)\n",
    "    map(m->mean(@where(sales_data, :Month.==m)[:Proportion]), unique_months)\n",
    "end\n",
    "\n",
    "average_sales_curve = get_sales_curve(sales_data, unique_months)\n",
    "\n",
    "plot(sales_data, :Month, :Proportion, group=:Season, xrotation=45,\n",
    "xlabel=\"Month\", ylabel=\"Proportion of Total Sales\")\n",
    "plot!(unique_months, average_sales_curve, label=\"Average Sales Curve\", linewidth=4)\n",
    "# savefig(\"salescurve.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting sales curve is very noisy, particularly in December where very low sales quantities were coducted in 2014. The low volume of sales was likely driven by the low prices observed at the time. However, since milk is a perishable product, the production still had to be sold at some point in time which accounts the the large spike in January and February when the product that was held back was released to the market. We could improve the sales curve by smoothing out these bumps, but for simplicity we shall use the unmodified version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(modelpath(\"sales_curve.json\"), \"w\") do file\n",
    "    JSON.print(file, map(r->round(r, 5), average_sales_curve))\n",
    "end\n",
    "\n",
    "function load_sales_curve()\n",
    "    JSON.parsefile(modelpath(\"sales_curve.json\"))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Estimating the product mix\n",
    "\n",
    "As part of the annual Milk Price Statement, Fonterra release their assumed production of each RCP broken down by quarter. We can use this data to calculate the quantity of each RCP produced from 1kg of milk solids.\n",
    "\n",
    "$$\\lambda_i = \\frac{Quantity\\ RCP_i\\ Produced}{Total\\ Production},\\quad i \\in \\{AMF, BUT, BMP, SMP, WMP\\}$$\n",
    "\n",
    "where $\\lambda_i$ is the quantity of RCP $i$ produced from 1kg of milk solids.\n",
    "\n",
    "This was done for all seasons and periods, and the average $\\lambda_i$ was calculated for each RCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_production_data()\n",
    "    qprod = getdata(\"fonterra/quarterly_production.csv\")\n",
    "\n",
    "    for key in [:WMP, :SMP, :BUT, :AMF, :BMP]\n",
    "        qprod[Symbol(\"$(key)p\")] = qprod[key] ./ qprod[:Supply]\n",
    "    end\n",
    "\n",
    "    periods = [\"Jul-Aug\", \"Sep-Nov\", \"Dec-Feb\", \"Mar-May\"]\n",
    "    qprod[:sPeriod] = \"\"\n",
    "    @byrow! qprod :sPeriod = periods[:Period]\n",
    "    qprod[:SeasonPeriod] = map((s,p)->string(s, \" \",p), qprod[:Season], qprod[:sPeriod])\n",
    "    qprod = @select(qprod, :Season, :sPeriod, :SeasonPeriod, :WMPp, :SMPp, :BUTp, :AMFp, :BMPp)\n",
    "    names!(qprod, [:Season, :sPeriod, :SeasonPeriod, :WMP, :SMP, :BUT, :AMF, :BMP])\n",
    "\n",
    "    melt(qprod, [:Season, :sPeriod, :SeasonPeriod])\n",
    "end\n",
    "\n",
    "production_data = get_production_data()\n",
    "\n",
    "plot(production_data, :SeasonPeriod, :value, group=:variable,\n",
    "xlabel=\"Time\", ylabel=\"kg Produced per kgMS\", xrotation=90)\n",
    "# savefig(\"productmix.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clear evidence that the product mix follows a seasonal cycle where relatively more Whole Milk Powder is produced, and relatively less Skim Milk Powder is produced, during March to August than during September to February. These differences are driven by limitations in capacity (during the peak of production in October/November, Fonterra may be constrained by the perishable nature of milk and forced to convert it into less valuable products), or differences in value over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sales_mix(production_data)\n",
    "    product_lambda = by(production_data, [:variable, :sPeriod]) do _df\n",
    "            mean(_df[:value]) \n",
    "        end\n",
    "    names!(product_lambda, [:product, :period, :weight])\n",
    "    product_lambda\n",
    "end\n",
    "product_mix = get_sales_mix(production_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values can be interpreted as follows: for every one kilogram of milk solids supplied to Fonterra during December and February, on average 1.15kg of WMP is produced, 0.42kg of SMP is produced and so on. These values sum to more than one because the input measurement (milk solids) does not include water, while the outputs (whole milk powder) does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(modelpath(\"product_mix.json\"), \"w\") do file\n",
    "    JSON.print(file, product_mix)\n",
    "end\n",
    "function load_product_mix()\n",
    "    pm = JSON.parsefile(modelpath(\"product_mix.json\")) \n",
    "    df = load_df_from_json(pm)\n",
    "    df[:product] = convert(DataArray{Symbol, 1}, df[:product])\n",
    "    df\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Estimating the Exchange Rate\n",
    "\n",
    "Fonterra receives large quantities of Foreign currency receipts due to the fact GDT auctions are conducted in USD. However in order to pay the farmers, it is forced to convert these recepits back into New Zealand dollars. To reduce the risk surrounding the exchange rate conversion, Fonterra uses a range of forward foreign exchange contracts and currency options contracts. This hedging policy reduces the impact of the volatility in the New Zealand dollar on the FMP, but introduces an additional complexity for external modellers.\n",
    "\n",
    "The only information that Fonterra releases about its achieved exchange rate (except in the Milk Price Statement at the end of each season) is one paragraph each year that reads as follows:\n",
    "\n",
    "\"As of 31 July [Current Year], Fonterra had foreign exchange contracts in place in respect to approximately [X] per cent of the USD equivalent operating cash flow exposure expected to impact on the Farmgate Milk Price for the [Next Season] Season. If the balance was hedged based on a spot exchange rate of [Y], the average USD:NZD conversion rate would be [Z] cents.\"\n",
    "\n",
    "This contains enough information to calculate the average exchange rate already hedged by Fonterra, however due to the complex nature of the forward and options contracts, this average rate may vary as the spot price varies.\n",
    "\n",
    "The average exchange rate achieved by Fonterra during a season is therefore: \n",
    "\n",
    "$$Average\\ Exchange\\ Rate = Z + (1-X)(\\alpha -Y),$$\n",
    "\n",
    "where $\\alpha$ is the spot price at which Fonterra exchanges the remainder of its foreign currency receipts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function update_exchange_rate_data()\n",
    "    page = reportdata = String(get(\"http://www.rbnz.govt.nz/statistics/b1\").data)\n",
    "\n",
    "    dates = match(r\"<th class=\\\"title\\\" style=\\\"width\\: 9.1\\%\\\"></th>(.+?)</tr>\"s, page)[1]\n",
    "    rex = r\"<em>(.+?)</em>\"\n",
    "    _keys = String[]\n",
    "    for m in matchall(rex, dates)\n",
    "        push!(_keys, match(rex, m)[1])\n",
    "    end\n",
    "\n",
    "    usd = match(r\"<th class=\\\"title\\\".+?>United States dollar</td>(.+?)</tr>\"s, page)[1]\n",
    "    rex = r\"<td.*?>(.+?)</td>\"\n",
    "    _values = Float64[]\n",
    "    for m in matchall(rex, usd)\n",
    "        push!(_values, parse(Float64, match(rex, m)[1]))\n",
    "    end\n",
    "\n",
    "    df = getdata(\"rbnz/exchange_rates.csv\")\n",
    "    for (_date, val) in zip(_keys, _values)\n",
    "        append!(df, DataFrame(Date=\"$(Dates.Date(_date, \"dd u yyyy\"))\", USD=val))\n",
    "    end\n",
    "    unique!(df)\n",
    "    writetable(datapath(\"rbnz/exchange_rates.csv\"), df)\n",
    "end\n",
    "update_exchange_rate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = getdata(\"rbnz/exchange_rates.csv\", :Date)\n",
    "# ex[:Date] = convert(DataArray{Date}, map(i->Date(i, \"yyyy-mm-dd\"), ex[:Date]))\n",
    "ex[:Month] = map(Dates.month, ex[:Date])\n",
    "ex[:Season] = 0\n",
    "for i=1:size(ex, 1)\n",
    "    if ex[i, :Month] >= 6\n",
    "        ex[i, :Month] -= 5\n",
    "        ex[i, :Season] = Dates.year(ex[i, :Date])\n",
    "    else\n",
    "        ex[i, :Month] += 7\n",
    "        ex[i, :Season] = Dates.year(ex[i, :Date]) -1\n",
    "    end\n",
    "end\n",
    "\n",
    "fonterra = DataFrame(\n",
    "Season   = reverse([2015,      2014,      2013,      2012]),\n",
    "actual   = reverse([0.7082,    0.7882,    0.8086,    0.7986]),\n",
    "spot     = reverse([0.6798,    0.7329,    0.8430,    0.8200]),\n",
    "fraction = reverse([0.67,      0.71,      0.67,      0.6]),\n",
    "assumed  = reverse([0.6603,    0.8490,    0.7993,    0.8089]),\n",
    "implied  = reverse([0.69,      0.826,     0.798,     0.78]),\n",
    "Date = reverse([Date(2015,6,1), Date(2014,6,1), Date(2013,6,1), Date(2012,6,1)]),\n",
    ")\n",
    "f2 = copy(fonterra[end, :])\n",
    "f2[:Date][end] += Dates.Year(1)\n",
    "append!(fonterra, f2)\n",
    "fonterra[:hedged] = (fonterra[:implied] - (1 - fonterra[:fraction]) .* fonterra[:assumed]) ./ fonterra[:fraction]\n",
    "\n",
    "# w = DataFrame(Month=1:14, weight=average_sales_curve)\n",
    "# w = DataFrame(Month=1:14, weight=ones(14)/14)\n",
    "\n",
    "rbnz = deepcopy(ex)\n",
    "rbnz2 = copy(@where(rbnz, :Month .<= 2))\n",
    "rbnz2[:Month] += 12\n",
    "rbnz2[:Season] -= 1\n",
    "append!(rbnz, rbnz2)\n",
    "\n",
    "# rbnz_mth = by(rbnz, [:Season, :Month]) do df\n",
    "#     DataFrame(USD = mean(df[:USD]))\n",
    "# end\n",
    "# rbnz_mth = join(rbnz_mth, w, on=[:Month])\n",
    "\n",
    "spot = by(rbnz, :Season) do df\n",
    "#     DataFrame(CalculatedSpot=dot(df[:USD], df[:weight]))\n",
    "#     DataFrame(CalculatedSpot=mean(df[:USD]))\n",
    "    DataFrame(CalculatedSpot=quantile(df[:USD], 0.5))\n",
    "end\n",
    "\n",
    "estimates = join(fonterra, spot, on=[:Season])\n",
    "\n",
    "estimates[:Estimated] = estimates[:fraction] .* estimates[:hedged] + (1-estimates[:fraction]) .* estimates[:CalculatedSpot]\n",
    "estimates[:Error] = estimates[:Estimated] - estimates[:actual]\n",
    "fonterra[:Estimated] = estimates[:Estimated]\n",
    "fonterra[:Estimated][end] = fonterra[:Estimated][end-1]\n",
    "estimates = @select(estimates, :Season, :Date, :actual, :Estimated, :Error, :CalculatedSpot)\n",
    "\n",
    "plot(@where(ex, :Date.>=Date(2012,6,1), :Date.<=Date(2016,6,1)), :Date, :USD, label=\"Spot Rate\")\n",
    "\n",
    "plot!(fonterra, :Date, :spot, linetype=:step, linewidth=2, label=\"Fonterra: Sales Avg. Spot\")\n",
    "plot!(estimates, :Date, :CalculatedSpot, linetype=:step, linewidth=2, label=\"Calculated: Sales Avg. Spot\")\n",
    "\n",
    "plot!(fonterra, :Date, :actual, linetype=:step, linewidth=3, label=\"Fonterra: Achieved Conversion Rate\")\n",
    "plot!(fonterra, :Date, :Estimated, linetype=:step, linewidth=3, label=\"Calculated: Average Conversion Rate\")\n",
    "plot!(ylabel=\"USD:NZD Exchange Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we have plotted the daily USD:NZD spot price over time, as well as the actual conversion rate achieved by Fonterra (reported _ex-post_) and the imputed average conversion rate. As a simplifying assumption, we assume that the average rate at which Fonterra converts the remainder of its unhedged foreign currency requirements is at the median spot price observed during the season.\n",
    "\n",
    "We have glossed over a complication that the percentage reported by Fonterra is based on their _expected_ foreign exchange requirements. Large deviations (due to differences in production or sales prices) will impact the reliability of that assumption. One anecdoetal case for this is the 2015/16 season, in which Fonterra reported in September having 67% of their expected foreign currecy needs hedged as of July 31st. Given their forecast at the time of \\$4.60 (compared to the final payout of \\$3.90), it is likely that they overestimated the quantity of foreign currecy they would need to exchange. Increasing the percentage already hedged increases our estimate of the final average conversion rate, bringing it closer to that reported by Fonterra.\n",
    "\n",
    "Estimating Fonterra's estimation of its future currency needs proved difficult so we leave that to future work. We also call on them to release additional information in a more timely fashion. In practice it is likely up to the practitioner to apply their own judgement as to whether it is likely that Fonterra misjudged the quantity of foreign currency it would need to exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immutable ExchangeRate{T} <: Distributions.Sampleable{Univariate,Continuous}\n",
    "    proportion::Distributions.Uniform{T}\n",
    "    spot::Distributions.Uniform{T}\n",
    "    original_proportion::T\n",
    "    hypothetical_spot::T\n",
    "    implied_exchange_rate::T\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An empirical estimator for the exchange rate\n",
    "function Base.rand{T}(ex::ExchangeRate{T})\n",
    "    proportion = rand(ex.proportion)\n",
    "    spot = rand(ex.spot)\n",
    "    alpha = (ex.implied_exchange_rate - (1-ex.original_proportion) * ex.hypothetical_spot) / ex.original_proportion\n",
    "    alpha * proportion + (1-proportion) * spot\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Estimating Fonterra's Cash and Capital Costs\n",
    "\n",
    "The fourth challenge is estimating Fonterra's cash and capital costs. Although the smallest component of the three, lactose exhibits the largest variability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_costs()\n",
    "    summary = getdata(\"fonterra/summary.csv\")\n",
    "    summary[:Total] = summary[:Lactose_Cost] + summary[:Cash_Costs] + summary[:Capital_Costs]\n",
    "    @select(summary, :Season, :Total, :Lactose_Cost,:Cash_Costs, :Capital_Costs)\n",
    "end\n",
    "\n",
    "fonterra_costs = get_costs()\n",
    "    \n",
    "plot(melt(fonterra_costs, :Season), :Season, :value, group=:variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were unable to find any predictive correlates for the total costs. We leave the improvement of this model to future work.\n",
    "\n",
    "A distribution for Fonterra's costs is the uniform distribution over $[1.93, 2.39]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_distribution = Uniform(minimum(fonterra_costs[:Total]), maximum(fonterra_costs[:Total]))\n",
    "open(modelpath(\"cost_distribution.json\"), \"w\") do file\n",
    "    JSON.print(file, cost_distribution)\n",
    "end\n",
    "\n",
    "# function load_cost_distribution()\n",
    "#     d = JSON.parsefile(modelpath(\"cost_distribution.json\")) \n",
    "# #     Uniform(d[\"a\"], d[\"b\"])\n",
    "#     TriangularDist(d[\"a\"], d[\"b\"], 2.05)\n",
    "# end\n",
    "function load_cost_distribution(thedate)\n",
    "    cost = getdata(\"forecasts/cost.csv\", :Date)\n",
    "    cost = @where(cost, :Season .== 2016, :Date .< thedate)\n",
    "    Uniform(cost[end, :Minimum], cost[end, :Maximum])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(a). Forecasting GDT Prices - using a random walk\n",
    "\n",
    "First we obtained the average sales price (USD/Tonne) for each RCP in GDT auctions between July 2010 and July 2016. Since both Butter and Butter Milk Powder were either not traded, or irregularly traded during this time, a linear regression was used to impute the values for which there was no data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_imputed_gdt()\n",
    "    gdt = getdata(\"gdt/events.csv\", :Date)\n",
    "#     gdt[:Date] = convert(DataArray{Date, 1}, map(d->Dates.Date(d, \"yyyy-m-d\"), gdt[:Date]))\n",
    "\n",
    "    # Impute BMP\n",
    "    rename!(gdt, :BMP, :BMPold)\n",
    "    bmp_train = @where(gdt, :BMPold .!= \"n.a.\")\n",
    "    bmp_train[:BMPold] = convert(DataArray{Float64}, map(s->parse(Float64, s), bmp_train[:BMPold]))\n",
    "#     bmp_model = fit(LinearModel, @formula(BMPold~AMF+SMP+WMP), bmp_train)\n",
    "    bmp_model = fit(LinearModel, BMPold~AMF+SMP+WMP, bmp_train)\n",
    "    \n",
    "    bmp_test = @where(gdt, :BMPold .== \"n.a.\")\n",
    "    gdt[:BMP] = 0.\n",
    "    gdt[:BMP][gdt[:BMPold] .== \"n.a.\"] = predict(bmp_model, bmp_test)\n",
    "    gdt[:BMP][gdt[:BMPold] .!= \"n.a.\"] = map(i->parse(Float64, i), gdt[:BMPold][gdt[:BMPold] .!= \"n.a.\"])\n",
    "\n",
    "    # Impute BUT\n",
    "    rename!(gdt, :BUT, :BUTold)\n",
    "    but_train = @where(gdt, :BUTold .!= \"n.a.\")\n",
    "    but_train[:BUTold] = convert(DataArray{Float64}, map(s->parse(Float64, s), but_train[:BUTold]))\n",
    "#     but_model = fit(LinearModel, @formula(BUTold~AMF+SMP+WMP), but_train)\n",
    "    but_model = fit(LinearModel, BUTold~AMF+SMP+WMP, but_train)\n",
    "    but_test = @where(gdt, :BUTold .== \"n.a.\")\n",
    "    gdt[:BUT] = 0.\n",
    "    gdt[:BUT][gdt[:BUTold] .== \"n.a.\"] = predict(but_model, but_test)\n",
    "    gdt[:BUT][gdt[:BUTold] .!= \"n.a.\"] = map(i->parse(Float64, i), gdt[:BUTold][gdt[:BUTold] .!= \"n.a.\"])\n",
    "    gdt\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the sales average price of milk in a GDT auction $y_t$ (USD/Tonne) as:\n",
    "\n",
    "$$y_t = \\sum\\limits_{i\\in I} \\lambda_i(t) P^{(i)}_t,$$\n",
    "\n",
    "where $I = \\{AMF, BUT, BMP, SMP, WMP\\}$, $\\lambda_i(t)$ is the product mix coefficient for RCP $i$ in GDT auction $t$ (calculated earlier) and $P^{(i)}_t$ is the average sales price per tonne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_spot_price()\n",
    "    gdt = get_imputed_gdt();\n",
    "    # product_lambda\n",
    "    product_mix = load_product_mix()\n",
    "    names!(product_mix, [:variable, :Month, :weight])\n",
    "\n",
    "    mth_groups = [\"Dec-Feb\",\"Dec-Feb\",\"Mar-May\",\n",
    "    \"Mar-May\",\"Mar-May\",\"Jul-Aug\",\n",
    "    \"Jul-Aug\",\"Jul-Aug\",\"Sep-Nov\",\n",
    "        \"Sep-Nov\",\"Sep-Nov\",\"Dec-Feb\"]\n",
    "    gdt[:Month] = map(m->mth_groups[Dates.month(m)], gdt[:Date])\n",
    "\n",
    "    product_mix[:variable] = convert(DataArray{Symbol, 1}, product_mix[:variable])\n",
    "\n",
    "    spot_price = by(\n",
    "        join(\n",
    "            melt(\n",
    "                @select(gdt, :TE, :Date, :Season, :Month, :AMF, :BUT, :BMP, :SMP, :WMP),\n",
    "                [:TE, :Date, :Season, :Month]\n",
    "            ),\n",
    "            product_mix,\n",
    "            on=[:Month, :variable]),\n",
    "        :Date) do df\n",
    "            dot(df[:value], df[:weight]) \n",
    "    end\n",
    "    names!(spot_price, [:Date, :SpotPrice])\n",
    "    spot_price\n",
    "end\n",
    "\n",
    "spot_price = get_spot_price()\n",
    "\n",
    "plot(spot_price, :Date, :SpotPrice, linetype=:step, legend=false,\n",
    "xlabel=\"Date\", ylabel=\"Product Mix Weighted GDT Price (USD/Tonne)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlogram(log.(collect(spot_price[:SpotPrice])), 5, partial=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot the partial auto-correlation coefficients, there is strong evidence of a log AR(2) effect.\n",
    "\n",
    "$$log(y_t) = \\alpha_0 + \\alpha_1 log(y_{y-1}) + \\alpha_2 log(y_{t-2}) + \\varepsilon_t,\\quad \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "We can fit the $\\alpha$ parameters using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdata = DataFrame(\n",
    "yt  = log.(spot_price[:SpotPrice][3:end]),\n",
    "yt1 = log.(spot_price[:SpotPrice][2:(end-1)]),\n",
    "yt2 = log.(spot_price[:SpotPrice][1:(end-2)])\n",
    ")\n",
    "# logmodel = fit(LinearModel, @formula(yt~yt1+yt2), logdata)\n",
    "logmodel = fit(LinearModel, yt~yt1+yt2, logdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider the qq-plot of the residuals (below), there is some evidence that the residuals are not normal. Indeed there is some evidence that the middle of the distribution has a smaller standard deviation that the standard deviation of all of the residuals, while the tails are fatter than could be expected from a normal distribution. One reason for this might be that the sequence of spot prices is typically characterised by small incremental changes, punctuated by large shocks to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(Normal(0, std(residuals(logmodel))), residuals(logmodel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution to the poor fit is to sample from the empirical distribution of errors rather than from the fitted Normal distribution.\n",
    "\n",
    "We now have a model for simulating future GDT prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immutable GDTAR2 <: Distributions.Sampleable{Univariate,Continuous}\n",
    "    coeffs::Vector{Float64}\n",
    "    sales_curve::Vector{Float64}\n",
    "    empirical_errors::Vector{Float64}\n",
    "    observations_to_date::Vector{Float64} # includes the previous two from the last season\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immutable GDTAR2b <: Distributions.Sampleable{Univariate,Continuous}\n",
    "    coeffs::Vector{Float64}\n",
    "    sales_curve::Vector{Float64}\n",
    "    empirical_errors::Vector{Float64}\n",
    "    observations_to_date::Vector{Float64} # includes the previous two from the last season\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function nextspot(yt1, yt2, coeffs, errs)\n",
    "    exp(coeffs[1] + coeffs[2] * log(yt1) + coeffs[3] * log(yt2) + rand(errs)) \n",
    "end\n",
    "function simulate_prices(t::Int, yt1::Float64, yt2::Float64,\n",
    "    coeffs::Vector{Float64}, w::Vector{Float64}, errs::Vector{Float64})\n",
    "    z = 0.0\n",
    "    for i=t:length(w)\n",
    "        y = nextspot(yt1, yt2, coeffs, errs)\n",
    "        z += y * w[i]\n",
    "        yt2 = yt1\n",
    "        yt1 = y\n",
    "    end\n",
    "    z\n",
    "end\n",
    "function Base.rand(g::GDTAR2)\n",
    "    t = length(g.observations_to_date)\n",
    "    (dot(g.observations_to_date, g.sales_curve[1:t]) +\n",
    "    simulate_prices(t+1, g.observations_to_date[end], g.observations_to_date[end-1], g.coeffs, g.sales_curve, g.empirical_errors))/1000.\n",
    "end\n",
    "function Base.rand(g::GDTAR2b)\n",
    "    t = length(g.observations_to_date) - 2\n",
    "    (dot(g.observations_to_date[3:end], g.sales_curve[1:t]) +\n",
    "    simulate_prices(t+1, g.observations_to_date[end], g.observations_to_date[end-1], g.coeffs, g.sales_curve, g.empirical_errors))/1000.\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(modelpath(\"ar2_coefficients.json\"), \"w\") do file\n",
    "    JSON.print(file, coef(logmodel))\n",
    "end\n",
    "load_ar2_coefficients() = JSON.parsefile(modelpath(\"ar2_coefficients.json\"))\n",
    "open(modelpath(\"empirical_errors.json\"), \"w\") do file\n",
    "    JSON.print(file, residuals(logmodel))\n",
    "end\n",
    "load_empirical_errors() = JSON.parsefile(modelpath(\"empirical_errors.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(b). Forecasting GDT Prices - using the NZX Futures\n",
    "\n",
    "A second source of information is available in order to forecast future GDT prices in NZX GDT Commodity Futures. These are financial futures contracts that settle against the average of the two GDT auctions in a month for AMF, BUT, SMP and WMP. Due to the small volumes, there is no contract for BMP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gettype(x)\n",
    "    if contains(x, \"/NZXWMP\")\n",
    "        return \"WMPfuture\"\n",
    "    elseif  contains(x, \"/NZXSMP\")\n",
    "        return \"SMPfuture\"\n",
    "    elseif  contains(x, \"/NZXAMF\")\n",
    "        return \"AMFfuture\"\n",
    "    elseif  contains(x, \"/NZXBTR\")\n",
    "        return \"BUTfuture\"\n",
    "    elseif  contains(x, \"/NZXMKP\")\n",
    "        return \"MKPfuture\"\n",
    "    elseif  contains(x, \"/WMP\")\n",
    "        return \"WMPoption\"\n",
    "    elseif  contains(x, \"/MKP\")\n",
    "        return \"MKPoption\"\n",
    "    end\n",
    "end\n",
    "\n",
    "function savereport(report)\n",
    "    reportdata = String(gethttp(\"nzxfutures.com/system/dsp_reports/$report\").data)\n",
    "    filename = match(r\".*/(.*?).csv\", report)[1]\n",
    "    open(\"$(filename).csv\", \"w\") do f\n",
    "        write(f, reportdata)\n",
    "    end\n",
    "    _df = readtable(\"$(filename).csv\")\n",
    "    rm(\"$(filename).csv\")\n",
    "    df = getdata(\"nzx/$(gettype(report)).csv\")\n",
    "    append!(df, _df)\n",
    "    unique!(df)\n",
    "    sort!(df, cols=[:Trade_date])\n",
    "    writetable(datapath(\"nzx/$(gettype(report)).csv\"), df)\n",
    "end\n",
    "\n",
    "gethttp(s) = get(\"http://$s\")\n",
    "\n",
    "function scrape_nzx()\n",
    "    pagedata = String(gethttp(\"nzxfutures.com/dairy/market_info\").data)\n",
    "    for report in matchall(r\"reports/([0-9]+?/original/\\w+?-[0-9]+?-final\\.csv)\", pagedata)\n",
    "        savereport(report)\n",
    "    end\n",
    "\n",
    "    df = getdata(\"nzx/MKPfuture.csv\")\n",
    "    mkp = @select(@where(df, :Code .== \"MKPFU17\"), :Trade_date, :Calculated_DSP)\n",
    "    names!(mkp, [:Date, :Forecast])\n",
    "    mkp[:Season] = 2016\n",
    "    writetable(datapath(\"forecasts/mkp.csv\"), mkp)\n",
    "end\n",
    "\n",
    "scrape_nzx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Requests\n",
    "\n",
    "function get_latest_futures()\n",
    "    full_df = getdata(\"nzx/final_futures.csv\")\n",
    "\n",
    "    pagedata = String(get(\"http://nzxfutures.com/dairy/market_info\").data)\n",
    "    reports = matchall(r\"downloads/([0-9]+?/NZX_(\\w+)_Futures_FSP.csv)\", pagedata)\n",
    "    for report in reports\n",
    "        filename = \"tmp.csv\"\n",
    "        open(filename, \"w\") do file\n",
    "            write(file, String(get(\"http://nzxfutures.com/system/$report\").data))\n",
    "        end\n",
    "        df = readtable(filename)\n",
    "        if isa(df[:Final_Settlement_Price], DataArray{String})\n",
    "            df[:Final_Settlement_Price] = convert(DataArray{Float64}, map(p->parse(Float64, replace(p, \",\", \"\")), df[:Final_Settlement_Price]))\n",
    "        end\n",
    "        r = df[:Settlement_Date] .== \"23/08/2016\"\n",
    "        if sum(r) > 0 # incorrect data\n",
    "            df[r, :Settlement_Date] = \"23/09/2016\"\n",
    "        end\n",
    "        rm(filename)\n",
    "        append!(full_df, df)\n",
    "        unique!(full_df)\n",
    "    sort!(full_df, cols=[:Settlement_Date])\n",
    "    writetable(datapath(\"nzx/final_futures.csv\"), full_df)\n",
    "    end\n",
    "end\n",
    "try\n",
    "    get_latest_futures()\n",
    "catch\n",
    "    warn(\"Unable to get latest futures\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function getfinalfutures()\n",
    "    df = getdata(\"nzx/final_futures.csv\")\n",
    "    df[:Code] = map(d->length(d)==6?(\"F$(d[4:end])\"):(d[4:end]), df[:Contract_Code])\n",
    "    df[:Date] = convert(DataArray{Date}, map(d->Dates.Date(d, \"dd/mm/yyyy\"), df[:Settlement_Date]))\n",
    "    df[:Prod] = map(d->d[1:3], df[:Contract_Code])\n",
    "    df = @select(df, :Code, :Date, :Prod, :Final_Settlement_Price)\n",
    "    df = unstack(df, :Prod, :Final_Settlement_Price)\n",
    "    rename!(df, :BTR, :BUT)\n",
    "    sort!(df, cols=[:Date])\n",
    "    df = df[isna.(df[:BUT]) .< 0.5, :]\n",
    "    df\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const codes = [\"FF\", \"FG\", \"FH\", \"FJ\", \"FK\", \"FM\", \"FN\", \"FQ\", \"FU\", \"FV\", \"FX\", \"FZ\"]\n",
    "function getcode(s)\n",
    "    for (idx, code) in enumerate(codes)\n",
    "        if code == s[4:5]\n",
    "            return idx\n",
    "        end\n",
    "    end\n",
    "    return -1\n",
    "end\n",
    "\n",
    "function getfutures(key)\n",
    "    df = getdata(\"nzx/$(key)future.csv\")\n",
    "    df[:Month] = 0\n",
    "    df[:Month] = convert(Vector{Int}, map(getcode, df[:Code]))\n",
    "    df[:Year] = 0\n",
    "    @byrow! df :Year = 2000 + parse(Int, :Code[(end-1):end])\n",
    "    df = @select(df, :Trade_date, :Month, :Year, :Calculated_DSP)\n",
    "    rename!(df, :Calculated_DSP, key)\n",
    "    df\n",
    "end\n",
    "\n",
    "function load_future_data()\n",
    "    product_mix = load_product_mix()\n",
    "    names!(product_mix, [:variable, :Month, :weight])\n",
    "    product_mix[:variable] = convert(DataArray{Symbol, 1}, product_mix[:variable])\n",
    "\n",
    "    product_lambda2 = join(product_mix, by(product_mix, :Month) do df\n",
    "        DataFrame(Multiplier = sum(df[:weight]) / sum(@where(df, :variable .!= Symbol(\"BMP\"))[:weight]))\n",
    "        end, on=[:Month])\n",
    "    product_lambda2[:weight] .*= product_lambda2[:Multiplier]\n",
    "    product_lambda2 = @select(product_lambda2, :Month, :variable, :weight)\n",
    "\n",
    "    historic_futures = getfinalfutures()#getdata(\"nzx/settled_futures.csv\", :Date)\n",
    "#     historic_futures[:Date] = convert(DataArray{Date, 1}, map(d->Dates.Date(d, \"yyyy-m-d\"), historic_futures[:Date]))\n",
    "    mth_groups = [\"Dec-Feb\",\"Dec-Feb\",\"Mar-May\",\n",
    "    \"Mar-May\",\"Mar-May\",\"Jul-Aug\",\n",
    "    \"Jul-Aug\",\"Jul-Aug\",\"Sep-Nov\",\n",
    "        \"Sep-Nov\",\"Sep-Nov\",\"Dec-Feb\"]\n",
    "    historic_futures[:Month] = map(m->mth_groups[Dates.month(m)], historic_futures[:Date])\n",
    "\n",
    "    historic_futures2 = join(melt(historic_futures, [:Code, :Date, :Month]), product_lambda2, on=[:variable, :Month])\n",
    "\n",
    "    historic_futures2[:Spot] = historic_futures2[:value] .* historic_futures2[:weight]\n",
    "    historic_spot = by(historic_futures2, :Date) do df\n",
    "        DataFrame(Spot = sum(df[:Spot]))\n",
    "    end\n",
    "    historic_spot[:Month] = map(Dates.month, historic_spot[:Date])\n",
    "    historic_spot[:Year] = map(Dates.year, historic_spot[:Date])\n",
    "    historic_spot[:SeasonMonth] = 0\n",
    "    historic_spot[:Season] = 0\n",
    "    for i=1:size(historic_spot, 1)\n",
    "        mth = historic_spot[i, :Month]\n",
    "        yr = historic_spot[i, :Year]\n",
    "        if mth < 6\n",
    "            historic_spot[i, :SeasonMonth] = mth + 7\n",
    "            historic_spot[i, :Season] = yr - 1\n",
    "        else\n",
    "            historic_spot[i, :SeasonMonth] = mth - 5\n",
    "            historic_spot[i, :Season] = yr\n",
    "        end\n",
    "    end\n",
    "    tmp = copy(@where(historic_spot, :SeasonMonth .<= 2))\n",
    "    tmp[:SeasonMonth] += 12\n",
    "    tmp[:Season] -= 1\n",
    "    append!(historic_spot, tmp)\n",
    "    sort!(historic_spot, cols=[:Season, :SeasonMonth])\n",
    "    historic_spot = @select(historic_spot, :Season, :SeasonMonth, :Spot)\n",
    "\n",
    "    futures = getfutures(:WMP)\n",
    "    for key in [:SMP, :AMF, :BUT]\n",
    "        futures = join(futures, getfutures(key), on=[:Trade_date, :Month, :Year])\n",
    "    end\n",
    "    futures[:Trade_date] = convert(DataArray{Date, 1}, map(d->Dates.Date(d, \"yyyy-m-d\"), futures[:Trade_date]))\n",
    "    futures[:MonthIdx] = futures[:Month]\n",
    "    futures[:Month] = map(m->mth_groups[m], futures[:MonthIdx])\n",
    "    futures2 = join(melt(futures, [:Trade_date, :Month, :MonthIdx, :Year]), product_lambda2, on=[:Month, :variable])\n",
    "    futures2[:Spot] = futures2[:value] .* futures2[:weight]\n",
    "    futures2 = by(futures2, [:Trade_date, :MonthIdx, :Year]) do df\n",
    "        DataFrame(Spot=sum(df[:Spot]))\n",
    "    end\n",
    "    futures2[:SeasonMonth] = 0\n",
    "    futures2[:Season] = 0\n",
    "    for i=1:size(futures2, 1)\n",
    "        mth = futures2[i, :MonthIdx]\n",
    "        yr = futures2[i, :Year]\n",
    "        if mth < 6\n",
    "            futures2[i, :SeasonMonth] = mth + 7\n",
    "            futures2[i, :Season] = yr - 1\n",
    "        else\n",
    "            futures2[i, :SeasonMonth] = mth - 5\n",
    "            futures2[i, :Season] = yr\n",
    "        end\n",
    "    end\n",
    "    tmp = copy(@where(futures2, :SeasonMonth .<= 2))\n",
    "    tmp[:SeasonMonth] += 12\n",
    "    tmp[:Season] -= 1\n",
    "    append!(futures2, tmp)\n",
    "    sort!(futures2, cols=[:Season, :SeasonMonth])\n",
    "    future_spot = @select(futures2, :Trade_date, :Season, :SeasonMonth, :Spot)\n",
    "    historic_spot, future_spot\n",
    "end\n",
    "\n",
    "function getseasonmonth(date)\n",
    "    mth = Dates.month(date)\n",
    "    yr = Dates.year(date)\n",
    "    if mth < 6\n",
    "        mth += 7\n",
    "        yr -= 1\n",
    "    else\n",
    "        mth -= 5\n",
    "    end\n",
    "    yr, mth\n",
    "end\n",
    "\n",
    "function get_historic_observations(historic_spot, date, season, minmonth)\n",
    "    @where(historic_spot, :Season .== season, :SeasonMonth .< minmonth)\n",
    "end\n",
    "\n",
    "function get_future_observations(future_spot, date, season)\n",
    "    #season, month = getseasonmonth(date)\n",
    "    last_trade_date = maximum(@where(future_spot, :Trade_date .<= date, :Season .== season)[:Trade_date])\n",
    "        @select(\n",
    "            @where(future_spot, :Season.==season, :Trade_date.==last_trade_date),\n",
    "            :Season, :SeasonMonth, :Spot\n",
    "    )\n",
    "end\n",
    "\n",
    "\n",
    "function getnzxforecast(thedate)\n",
    "    historic_spot, future_spot = load_future_data()\n",
    "\n",
    "    nzx_data = get_future_observations(future_spot, thedate, 2016)\n",
    "    \n",
    "    append!(nzx_data, get_historic_observations(historic_spot, thedate, 2016, minimum(nzx_data[:SeasonMonth])))\n",
    "\n",
    "    nzx_observations = join(nzx_data, DataFrame(SeasonMonth=1:14, weight=load_sales_curve()), on=[:SeasonMonth])\n",
    "    dot(nzx_observations[:Spot], nzx_observations[:weight]) / 1000\n",
    "end\n",
    "\n",
    "thedate = Dates.today()\n",
    "getnzxforecast(thedate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_spot, future_spot = load_future_data()\n",
    "\n",
    "    nzx_data = get_future_observations(future_spot, thedate, 2016)\n",
    "    \n",
    "#     append!(nzx_data, get_historic_observations(historic_spot, thedate, minimum(nzx_data[:SeasonMonth])))\n",
    "\n",
    "#     nzx_observations = join(nzx_data, DataFrame(SeasonMonth=1:14, weight=load_sales_curve()), on=[:SeasonMonth])\n",
    "#     dot(nzx_observations[:Spot], nzx_observations[:weight]) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = getfinalfutures()# getdata(\"nzx/settled_futures.csv\", :Date)\n",
    "# convertdate!(futures, :Date)\n",
    "futures[:FirstDate] = map(d->Dates.Date(Dates.year(d), Dates.month(d), 1), futures[:Date])\n",
    "futures = melt(futures, [:Code, :FirstDate, :Date])\n",
    "futures[:Code] = map((v, c)->\"$v$c\", futures[:variable], futures[:Code])\n",
    "futures = @select(futures, :Code, :Date, :variable, :value)\n",
    "rename!(futures, :value, :Price)\n",
    "rename!(futures, :variable, :Product)\n",
    "futures[:Price] = convert(DataArray{Float64}, futures[:Price])\n",
    "\n",
    "nzxproductmonths = ['F', 'G', 'H', 'J', 'K', 'M', 'N', 'Q', 'U', 'V', 'X', 'Z']\n",
    "function getfuturesforecast(product)\n",
    "    df = getdata(\"nzx/$(product)future.csv\", :Trade_date)\n",
    "#     convertdate!(df, :Trade_date)\n",
    "    df[:Month] = map(c->findfirst(nzxproductmonths, c[5]), df[:Code])\n",
    "    df[:Year] = map(c->2000+parse(Int, c[6:7]), df[:Code])\n",
    "    df[:Date] = map((y,m)->Dates.Date(y, m, 1), df[:Year], df[:Month])\n",
    "    df[:Product] = convert(DataArray{Symbol}, map(d->Symbol(d[1:3]), df[:Code]))\n",
    "    maxdate = maximum(df[:Trade_date])\n",
    "    _df = @select(@where(df, :Trade_date .== maxdate), :Code, :Date, :Product, :Calculated_DSP)\n",
    "    rename!(_df, :Calculated_DSP, :Price)\n",
    "    _df\n",
    "end\n",
    "futureforecast = getfuturesforecast(\"WMP\")\n",
    "for product in [\"SMP\", \"AMF\", \"BUT\"]\n",
    "    append!(futureforecast, getfuturesforecast(product))\n",
    "end\n",
    "futureforecast[:Code] = map(c->replace(c, \"BTR\", \"BUT\"),futureforecast[:Code])\n",
    "futureforecast[:Product] = map(c->Symbol(replace(\"$c\", \"BTR\", \"BUT\")),futureforecast[:Product])\n",
    "\n",
    "data = Dict()\n",
    "for product in unique(futures[:Product])\n",
    "    data[product] = Dict()\n",
    "    data[product][\"historic\"] = Dict()\n",
    "    _ = @where(futures, :Product .== product)\n",
    "    sort!(_, cols=[:Date])\n",
    "    data[product][\"historic\"][\"x\"] = collect(_[:Date])\n",
    "    data[product][\"historic\"][\"price\"] = collect(_[:Price])\n",
    "    \n",
    "    data[product][\"forecast\"] = Dict()\n",
    "    _ = @where(futureforecast, :Product .== product)\n",
    "    sort!(_, cols=[:Date])\n",
    "    data[product][\"forecast\"][\"x\"] = collect(_[:Date])\n",
    "    data[product][\"forecast\"][\"price\"] = collect(_[:Price])\n",
    "end\n",
    "open(docpath(\"json/futures.json\"), \"w\") do file\n",
    "    print(file, \"future_data = \")\n",
    "    JSON.print(file, data)\n",
    "    print(file, \";\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An ensemble forecast\n",
    "\n",
    "We can then combine the following probabilistic forecasts:\n",
    "\n",
    "1. (5%)  AgriHQ\n",
    "2. (5%)  ASB\n",
    "3. (5%)  BNZ\n",
    "4. (5%)  NZX MKP\n",
    "5. (5%)  Westpac\n",
    "6. (25%) Fonterra Forecast\n",
    "7. (25%) Log-normal AR(2) of GDT spot\n",
    "8. (25%) NZX Futures Forecast\n",
    "\n",
    "into an ensemble forecast. Provided the errors between them are independent, the average of these forecasts should outperform any individual forecast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function getanalyst_quantiles(analyst, season, date, N, quants)\n",
    "    dist = getanalystforecast(analyst, season, date)\n",
    "    quantile(rand(dist, N), quants)\n",
    "end\n",
    "\n",
    "function calculate_forecasts(thedate, exchange_rate_distribution, N, quants, saveforecasts=true)\n",
    "    # Fonterra Estimates\n",
    "    fonterra_distribution = getanalystforecast(\"fonterra\", 2016, thedate)\n",
    "    fonterra_quantiles = quantile(rand(fonterra_distribution, N), quants)\n",
    "    \n",
    "    # Analyst estimates\n",
    "    agrihq_quantiles  = getanalyst_quantiles(\"agrihq\", 2016, thedate, N, quants)\n",
    "    asb_quantiles     = getanalyst_quantiles(\"asb\", 2016, thedate, N, quants)\n",
    "    bnz_quantiles     = getanalyst_quantiles(\"bnz\", 2016, thedate, N, quants)\n",
    "    mkp_quantiles     = getanalyst_quantiles(\"mkp\", 2016, thedate, N, quants)\n",
    "    westpac_quantiles = getanalyst_quantiles(\"westpac\", 2016, thedate, N, quants)\n",
    "    \n",
    "    # Cost distribution\n",
    "    cost_distribution = load_cost_distribution(thedate)\n",
    "\n",
    "    # Draw N exchange rate samples\n",
    "    exchange_rate_samples = rand(exchange_rate_distribution, N)\n",
    "    \n",
    "    # Draw N cost samples\n",
    "    cost_samples = rand(cost_distribution, N)\n",
    "\n",
    "    # GDT AR(2) estimator\n",
    "    spot_price = get_spot_price()\n",
    "    gdt_ar2_estimator = GDTAR2b(\n",
    "        load_ar2_coefficients(),\n",
    "        [wi/2 for wi in load_sales_curve() for i=1:2],\n",
    "        load_empirical_errors(),\n",
    "        collect(@where(spot_price, :Date .>= Dates.Date(2016,5,1), :Date .<= thedate)[:SpotPrice])\n",
    "    )\n",
    "\n",
    "    # NZX forecast\n",
    "    nzx_futures_forecast = getnzxforecast(thedate)\n",
    "    @show nzx_futures_forecast\n",
    "\n",
    "    # Estimate GDT season average by autoregressive random walk\n",
    "    autoregressive_gdt_forecasts = rand(gdt_ar2_estimator, N)\n",
    "    autoregressive_forecasts = autoregressive_gdt_forecasts ./ exchange_rate_samples - cost_samples\n",
    "    autoregressive_quantiles = quantile(autoregressive_forecasts, quants)\n",
    "    \n",
    "    # Use same distribution but shifted to NZX futures mean estimate\n",
    "    nzx_futures_gdt_forecasts = autoregressive_gdt_forecasts + nzx_futures_forecast - mean(autoregressive_gdt_forecasts)\n",
    "    nzx_futures_forecasts = nzx_futures_gdt_forecasts ./ exchange_rate_samples - cost_samples\n",
    "    nzx_quantiles = quantile(nzx_futures_forecasts, quants)\n",
    "    \n",
    "    if saveforecasts\n",
    "        append_auction_forecast(datapath(\"model_parameters/agrihq_forecasts.csv\"), thedate, 2016,agrihq_quantiles)\n",
    "        append_auction_forecast(datapath(\"model_parameters/asb_forecasts.csv\"), thedate, 2016,asb_quantiles)\n",
    "        append_auction_forecast(datapath(\"model_parameters/bnz_forecasts.csv\"), thedate, 2016,bnz_quantiles)\n",
    "        append_auction_forecast(datapath(\"model_parameters/mkp_forecasts.csv\"), thedate, 2016,mkp_quantiles)\n",
    "        append_auction_forecast(datapath(\"model_parameters/westpac_forecasts.csv\"), thedate, 2016,westpac_quantiles)\n",
    "        append_auction_forecast(datapath(\"model_parameters/fonterra_forecasts.csv\"), thedate, 2016,fonterra_quantiles)\n",
    "        append_auction_forecast(datapath(\"model_parameters/nzx_forecasts.csv\"), thedate, 2016,nzx_quantiles)\n",
    "        append_auction_forecast(datapath(\"model_parameters/ar_forecasts.csv\"), thedate, 2016,autoregressive_quantiles)\n",
    "    end\n",
    "    auction_plus_quantiles = \n",
    "        0.0 * agrihq_quantiles + \n",
    "        0.05 * asb_quantiles + \n",
    "        0.05 * bnz_quantiles + \n",
    "        0.0 * mkp_quantiles + \n",
    "        0.0 * westpac_quantiles + \n",
    "        0.5 * fonterra_quantiles +\n",
    "        0.2 * nzx_quantiles + \n",
    "        0.2 * autoregressive_quantiles\n",
    "    return auction_plus_quantiles\n",
    "end\n",
    "\n",
    "function create_json(filename, cols)\n",
    "    df = readtable(datapath(\"model_parameters/$(filename).csv\"))\n",
    "    outdict = Dict{Any, Any}()\n",
    "    for sym in names(df)[3:end]\n",
    "        outdict[sym] = Dict{Symbol, Any}(\n",
    "            :x => df[:Date],\n",
    "            :y => df[sym]\n",
    "        )\n",
    "    end\n",
    "    open(docpath(\"json/$(filename).json\"), \"w\") do file\n",
    "        print(file, \"$(filename) = \")\n",
    "        JSON.print(file, outdict)\n",
    "        print(file, \";\\n\")\n",
    "    end\n",
    "end\n",
    "\n",
    "function append_auction_forecast(filename, thedate, season, values)\n",
    "    open(filename, \"a\") do file\n",
    "        print(file, \"\\\"$(thedate)\\\",$(season)\")\n",
    "        for v in values\n",
    "            print(file, \",$(round(v, 2))\")\n",
    "        end\n",
    "        print(file, \"\\n\")\n",
    "    end\n",
    "end\n",
    "\n",
    "function save_futures_distribution(x)\n",
    "    open(docpath(\"json/future_returns.json\"), \"w\") do file\n",
    "        write(file, \"returns = \")\n",
    "        JSON.print(file, map(i->round(i, 3), x))\n",
    "        write(file, \";\\n\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current date\n",
    "thedate = Dates.today()\n",
    "\n",
    "# Estimate for exchange rate\n",
    "exchange_rate_distribution = ExchangeRate(Uniform(0.3, 0.5), Uniform(0.7, 0.72), 0.7, 0.7069, 0.68)\n",
    "\n",
    "auction_plus_forecast_quantiles = calculate_forecasts(\n",
    "                                                    thedate,\n",
    "                                                    exchange_rate_distribution,\n",
    "                                                    20000,\n",
    "                                                    [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile(rand(exchange_rate_distribution, 1000), [0.1, 0.25, 0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function getauctionplusquantiles(thedate)\n",
    "#     # Estimate for exchange rate\n",
    "#     if thedate < Dates.Date(2016,9,30)\n",
    "#         exchange_rate_distribution = Uniform(0.65, 0.73)\n",
    "#     else\n",
    "#         exchange_rate_distribution = ExchangeRate(Uniform(0.4, 0.6), Uniform(0.69, 0.71), 0.7, 0.7069, 0.68)\n",
    "#     end\n",
    "#     calculate_forecasts(\n",
    "#         thedate,\n",
    "#         exchange_rate_distribution,\n",
    "#         20000,\n",
    "#         [0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "#         false\n",
    "#     )\n",
    "# end\n",
    "\n",
    "# srand(11111)\n",
    "# auctionplus = getauctionplusquantiles(Dates.Date(2016,6,2))' # hack first obs.\n",
    "# srand(11111)\n",
    "# for thedate in Dates.Date(2016,6,2):Dates.today()\n",
    "#     Dates.day(thedate) == 1 && @show thedate\n",
    "#     auctionplus = vcat(auctionplus, getauctionplusquantiles(thedate)')\n",
    "# end\n",
    "# plotribbon!(x, l, u, c, lab) = plot!(x, (l+u)/2, ribbon=(u-l)/2, c=c, label=lab)\n",
    "\n",
    "# x = Dates.Date(2016,6,1):Dates.today()\n",
    "# theme(:none)\n",
    "\n",
    "# plot(\n",
    "#     title=\"Historic Forecasts of Auction-Plus Model\",\n",
    "#     xlabel=\"Date of Forecast\",\n",
    "#     ylabel=\"Milk Price (\\$/kgMS)\",\n",
    "#     ylims=(2, 9)\n",
    "# )\n",
    "\n",
    "# plotribbon!(x, auctionplus[:,1], auctionplus[:,5], colorant\"lightgrey\", \"80\\% Band (/10 - /90)\")\n",
    "# plotribbon!(x, auctionplus[:,2], auctionplus[:,4], colorant\"darkgrey\", \"50\\% Band (/25 - /75)\")\n",
    "# plot!(x, auctionplus[:,3], label=\"Median Guess (/50)\", c=colorant\"black\")\n",
    "# # savefig(\"auction_plus.pdf\")\n",
    "# open(datapath(\"model_parameters/auction_plus_forecasts.csv\"), \"w\") do file\n",
    "#     println(file, \"\\\"Date\\\",\\\"Season\\\",\\\"x10_Percentile\\\",\\\"x25_Percentile\\\",\\\"x50_Percentile\\\",\\\"x75_Percentile\\\",\\\"x90_Percentile\\\"\")\n",
    "# end\n",
    "# for i in 1:length(x)\n",
    "#     append_auction_forecast(datapath(\"model_parameters/auction_plus_forecasts.csv\"), x[i], 2016, auctionplus[i,:])\n",
    "# end\n",
    "# create_json(\"auction_plus_forecasts\", vcat(1,3:7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_auction_forecast(datapath(\"model_parameters/auction_plus_forecasts.csv\"), thedate, 2016,auction_plus_forecast_quantiles)\n",
    "create_json(\"auction_plus_forecasts\", vcat(1,3:7))\n",
    "# if NEW_AUCTION_ONLY\n",
    "#     append_auction_forecast(datapath(\"model_parameters/auction_only_forecasts.csv\"), thedate, 2016,auction_only_forecast_quantiles)\n",
    "#     create_json(\"auction_only_forecasts\", vcat(1,3:7))\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auction_plus_forecast_quantiles_b = calculate_forecasts(\n",
    "                                                    thedate,\n",
    "                                                    exchange_rate_distribution,\n",
    "                                                    20000,\n",
    "                                                    linspace(0, 1, 5000),\n",
    "                                                    false\n",
    "                                                )\n",
    "save_futures_distribution(rand(auction_plus_forecast_quantiles_b, 20000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the other forecasters\n",
    "\n",
    "This section is just helper functions to update the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function analyst_dictionary(filename, name, year=2016)\n",
    "    df = @where(readtable(filename), :Season .== year)\n",
    "    d = Dict{Symbol, Any}()\n",
    "    d[:name] = name\n",
    "    d[:x] = df[:Date]\n",
    "    d[:y] = df[:Forecast]\n",
    "    return d\n",
    "end\n",
    "\n",
    "function rebuild_other_analysts()\n",
    "    open(docpath(\"json/other_analysts.json\"), \"w\") do file\n",
    "        write(file, \"other_analysts = \")\n",
    "        JSON.print(file,\n",
    "            [\n",
    "            analyst_dictionary(datapath(\"forecasts/agrihq.csv\"), \"AgriHQ\"),\n",
    "            analyst_dictionary(datapath(\"forecasts/asb.csv\"), \"ASB\"),\n",
    "            analyst_dictionary(datapath(\"forecasts/bnz.csv\"), \"BNZ\"),\n",
    "            analyst_dictionary(datapath(\"forecasts/fonterra.csv\"), \"Fonterra\"),\n",
    "            analyst_dictionary(datapath(\"forecasts/mkp.csv\"), \"NZX:MKPFU17\"),\n",
    "            analyst_dictionary(datapath(\"forecasts/westpac.csv\"), \"Westpac\")\n",
    "            ]\n",
    "        )\n",
    "        write(file, \";\")\n",
    "    end\n",
    "    nothing\n",
    "end\n",
    "rebuild_other_analysts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rebuild_future_params()\n",
    "    open(docpath(\"json/future_params.json\"), \"w\") do file\n",
    "        latest_price = analyst_dictionary(datapath(\"forecasts/mkp.csv\"), \"NZX:MKPFU17\")[:y][end]\n",
    "        println(file, \"hedge_quantity = 50;\")\n",
    "        println(file, \"hedge_price    = $(latest_price);\")\n",
    "    end\n",
    "    nothing\n",
    "end\n",
    "rebuild_future_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rebuild_spot()\n",
    "    spot_price = get_spot_price()\n",
    "    open(docpath(\"json/spot_price.json\"), \"w\") do file\n",
    "        write(file, \"spot_price = \")\n",
    "        JSON.print(file,\n",
    "            Dict(\n",
    "                :name => \"Spot Price\",\n",
    "                :x => spot_price[:Date],\n",
    "                :y => round(spot_price[:SpotPrice], 2)\n",
    "            )\n",
    "        )\n",
    "        write(file, \";\\n\")\n",
    "    end\n",
    "    nothing\n",
    "end\n",
    "rebuild_spot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rebuild_fonterra_data()\n",
    "    df = getdata(\"forecasts/fonterra.csv\")\n",
    "    output = []\n",
    "    by(df, :Season) do f\n",
    "        push!(output, Dict{Symbol, Any}(:x=>f[:Date], :y=>f[:Forecast], :season=>f[:Season][1]))\n",
    "        return 0\n",
    "    end\n",
    "    output\n",
    "    open(docpath(\"json/fonterra_forecasts.json\"), \"w\") do file\n",
    "        write(file, \"fonterra_data = \")\n",
    "        JSON.print(file, output)\n",
    "        write(file, \";\")\n",
    "    end\n",
    "    nothing\n",
    "end\n",
    "rebuild_fonterra_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDT Forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nzx_months = ['F', 'G', 'H', 'J', 'K', 'M', 'N', 'Q', 'U', 'V', 'X', 'Z']\n",
    "\n",
    "# return the date of the n'th weekday in month-year\n",
    "#   defaults to weekday=2 (Tuesday)\n",
    "function getnthday(year, month, n, weekday=2)\n",
    "    # loop through seven days\n",
    "    for day in 1:7\n",
    "        d = Dates.Date(year, month, day + (n-1)*7) \n",
    "        if Dates.dayofweek(d) == weekday\n",
    "            return d\n",
    "        end\n",
    "    end\n",
    "    # error to preserve type stability\n",
    "    error(\"We couldn't find the right day in the week.\")\n",
    "end\n",
    "\n",
    "# this errors for a few auctions \n",
    "# i.e. TE27 is actually a Wednesday...\n",
    "# either errors in the data (from the Excel spreadsheet downloaded from GlobalDairyTrade\n",
    "# or they shifted somes auctions if the 1st of the month was on a Wednesday...\n",
    "# lets just ignore the differences for now\n",
    "function getdateofauction(n)\n",
    "    # we know TE27 happened on September 1st, 2010\n",
    "    TE27 = Dates.Date(2010,9,1)\n",
    "    if n < 27\n",
    "        # before TE27, auctions happened on the first Tuesday only\n",
    "        # calculate the approximate date (i.e. Year-Month) of the n'th TE\n",
    "        te_nth = TE27 + Dates.Month(n - 27)\n",
    "        # return the date\n",
    "        return getnthday(Dates.year(te_nth), Dates.month(te_nth), 1, 2)\n",
    "    else\n",
    "        # odd numbered TE's happen on the first tuesday\n",
    "        # even numbered TE's happen on the third tuesday\n",
    "        nth = isodd(n)?1:3\n",
    "        # calculate the approximate date (i.e. Year-Month) of the n'th TE\n",
    "        #  we step forward 1 month every two auctions. But TE27 was the first\n",
    "        #  auction in a month so we round down\n",
    "        te_nth = TE27 + Dates.Month(floor(Int, (n - 27)/2))\n",
    "        # return the date\n",
    "        return getnthday(Dates.year(te_nth), Dates.month(te_nth), nth, 2)\n",
    "    end\n",
    "end\n",
    "\n",
    "immutable EmpiricalError{T}\n",
    "    x::Vector{T}\n",
    "end\n",
    "Base.rand(er::EmpiricalError) = rand(er.x)\n",
    "immutable AR{N, T, T1, D}\n",
    "    c::Tuple{Vararg{T, T1}}\n",
    "    err::D\n",
    "end\n",
    "AR{T}(N::Int, c::Vector{T}, err) = AR{N, T, length(c), typeof(err)}(tuple(c...), err)\n",
    "\n",
    "function step{N,T, M}(m::AR{N, T, M}, x0::AbstractVector{T})\n",
    "    @assert length(x0) == N\n",
    "    y = m.c[1] + rand(m.err)\n",
    "    @inbounds for i in 1:N\n",
    "        y += m.c[i+1] * x0[i]\n",
    "    end\n",
    "    y \n",
    "end\n",
    "function simulatelog!{N,T, M}(y, m::AR{N, T, M}, x0::AbstractVector{T}, n)\n",
    "    @assert length(x0) == N\n",
    "    x = copy(x0)\n",
    "    newval = zero(T)\n",
    "    @inbounds for i in 1:n\n",
    "        newval = step(m, x)\n",
    "        for j in 2:N\n",
    "            x[j] = x[j-1]\n",
    "        end\n",
    "        x[1] = newval\n",
    "        y[i] = exp(newval)\n",
    "    end\n",
    "    y\n",
    "end\n",
    "function simulatelog{N,T, M}(m::AR{N, T, M}, x0::AbstractVector{T}, n)\n",
    "    y = zeros(T, n)\n",
    "    simulatelog!(y, m, x0, n)\n",
    "    y\n",
    "end\n",
    "function lag(df, lag, id, fixedcols)\n",
    "    new_df = copy(df)\n",
    "    for i in 1:lag\n",
    "        dfi = copy(df)\n",
    "        dfi[id] += i\n",
    "        new_df = join(new_df, dfi, on=[id])\n",
    "        for col in fixedcols\n",
    "            delete!(new_df, Symbol(\"$(col)_1\"))\n",
    "        end\n",
    "    end\n",
    "    new_df\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdt = @select(getdata(\"gdt/events.csv\"), :TE, :SMP, :WMP)\n",
    "sort!(gdt, cols=[:TE])\n",
    "gdt[:id] = 1:size(gdt, 1)\n",
    "gdt[:WMP] = log.(gdt[:WMP])\n",
    "gdt[:SMP] = log.(gdt[:SMP])\n",
    "lagged_prices = lag(gdt, 2, :id, [:TE])\n",
    "\n",
    "# wmp_model = fit(LinearModel, @formula(WMP ~ WMP_1 + WMP_2), lagged_prices)\n",
    "wmp_model = fit(LinearModel, WMP ~ WMP_1 + WMP_2, lagged_prices)\n",
    "wmp_ar = AR(2, coef(wmp_model), residuals(wmp_model))\n",
    "# smp_model = fit(LinearModel, @formula(SMP ~ SMP_1 + SMP_2), lagged_prices)\n",
    "smp_model = fit(LinearModel, SMP ~ SMP_1 + SMP_2, lagged_prices)\n",
    "smp_ar = AR(2, coef(smp_model), residuals(smp_model))\n",
    "\n",
    "function adddata!(data, model, x0, lastauction)\n",
    "    n = 20_000\n",
    "    Y = Array(Float64, (24, n))\n",
    "    for i in 1:n\n",
    "        simulatelog!(view(Y,:, i), model, x0, 24)\n",
    "    end\n",
    "    Z = Array(Float64, (size(Y, 1), 3))\n",
    "    for i in 1:size(Y, 1)\n",
    "        Z[i, :] = quantile(Y[i, :], [0.1, 0.5, 0.9])\n",
    "    end\n",
    "    data[\"date\"]   = [getdateofauction(lastauction + i) for i in 1:size(Y, 1)]\n",
    "    data[\"lower\"]  = round.(Z[:, 1], 2)\n",
    "    data[\"middle\"] = round.(Z[:, 2], 2)\n",
    "    data[\"upper\"]  = round.(Z[:, 3], 2)\n",
    "    \n",
    "end\n",
    "lastauction = maximum(gdt[:TE])\n",
    "data = Dict()\n",
    "models = Dict(\"WMP\" => wmp_ar, \"SMP\" => smp_ar)\n",
    "for prod in [\"WMP\", \"SMP\"]\n",
    "    data[prod] = Dict()\n",
    "    adddata!(data[prod], models[prod], reverse(collect(gdt[(end-1):end, Symbol(prod)])), lastauction)\n",
    "end\n",
    "open(docpath(\"json/future_price_forecast.json\"), \"w\") do file\n",
    "    println(file, \"future_price_forecast = \")\n",
    "    JSON.print(file, data)\n",
    "    println(file, \";\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = @select(getdata(\"nzx/dsp_complete.csv\", :Date, \"dd/mm/yyyy\"), :Contract, :Daily_Settlement_Price, :Date)\n",
    "# rename!(df, Dict(:Date=>:Trade_date, :Contract => :Code, :Daily_Settlement_Price=>:Calculated_DSP))\n",
    "\n",
    "# new_df = @select(getdata(\"nzx/WMPfuture.csv\", :Trade_date), :Code, :Calculated_DSP, :Trade_date);\n",
    "# append!(new_df, @select(getdata(\"nzx/SMPfuture.csv\", :Trade_date), :Code, :Calculated_DSP, :Trade_date))\n",
    "\n",
    "# append!(df, new_df)\n",
    "\n",
    "# df=df[isna.(df[:Calculated_DSP]) .< 0.5, :]\n",
    "\n",
    "# df[:Product] = map(s->s[1:4], df[:Code])\n",
    "# df = df[(df[:Product] .== \"WMPF\") | (df[:Product] .== \"SMPF\"), :]\n",
    "# df[:Year] = map(s->parse(Int, s[6:end])+2000, df[:Code])\n",
    "# df[:Month] = map(s->findin(nzx_months, s[5])[1], df[:Code])\n",
    "\n",
    "# # get third wednesday\n",
    "# df[:SettlementDate] = map((y,m)->getnthday(y, m, 3, 3), df[:Year], df[:Month])\n",
    "# df = @where(df, :Trade_date .<= :SettlementDate)\n",
    "\n",
    "# completed_df = df[(df[:Year] .< 2017) | (df[:Month] .< Dates.month(Dates.today())), :]\n",
    "\n",
    "# finalprice = by(completed_df, :Code) do tmp\n",
    "#     lasttradedate = maximum(tmp[:Trade_date])\n",
    "#     DataFrame(\n",
    "#     FinalDate = lasttradedate,\n",
    "#     FinalPrice=@where(tmp, :Trade_date .== lasttradedate)[:Calculated_DSP][1]\n",
    "#     )\n",
    "# end\n",
    "# completed_df = join(completed_df, finalprice, on=:Code)\n",
    "# completed_df = @select(completed_df, :Product, :Year, :Month, :Code, :Trade_date, :Calculated_DSP, :FinalDate, :FinalPrice)\n",
    "# sort!(completed_df, cols=[:Product, :Code, :Trade_date])\n",
    "# completed_df[:DateDistance] = map((d1, d2) -> round(Int, (d1 - d2).value), completed_df[:FinalDate], completed_df[:Trade_date])\n",
    "# completed_df[:Error] = completed_df[:FinalPrice] - completed_df[:Calculated_DSP]\n",
    "# completed_df[:WeekDistance] = round(Int, completed_df[:DateDistance] / 7)\n",
    "# unique!(completed_df)\n",
    "# completed_df[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(by(@where(completed_df, :WeekDistance .<= 30), [:FinalDate, :Product, :Code]) do _\n",
    "#     sum((_[:FinalPrice]./_[:Calculated_DSP]-1).^2) / size(_, 1)\n",
    "#     end,\n",
    "# :FinalDate, :x1, group=:Product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# historical_accuracy = by(completed_df, [:Product, :WeekDistance]) do tmp\n",
    "#     DataFrame(\n",
    "#     lower = quantile(tmp[:Error], 0.1),\n",
    "#     middle = quantile(tmp[:Error], 0.5),\n",
    "#     upper = quantile(tmp[:Error], 0.9)\n",
    "#     )\n",
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
